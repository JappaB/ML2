{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a10081f57b90a368eb8daf62e3ba00e",
     "grade": false,
     "grade_id": "cell-02487845739eb4fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Lab 3: Expectation Maximization and Variational Autoencoder\n",
    "\n",
    "### Machine Learning 2 (2017/2018)\n",
    "\n",
    "* The lab exercises should be made in groups of two or three people.\n",
    "* The deadline is Friday, 01.06.\n",
    "* Assignment should be submitted through BlackBoard! Make sure to include your and your teammates' names with the submission.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab#\", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the ```# YOUR CODE HERE``` comment and overwrite the ```raise NotImplementedError()``` line.\n",
    "    * For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All.\n",
    "$\\newcommand{\\bx}{\\mathbf{x}} \\newcommand{\\bpi}{\\mathbf{\\pi}} \\newcommand{\\bmu}{\\mathbf{\\mu}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\bZ}{\\mathbf{Z}} \\newcommand{\\bz}{\\mathbf{z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4e05229ee79b55d6589e1ea8de68f32",
     "grade": false,
     "grade_id": "cell-a0a6fdb7ca694bee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Installing PyTorch\n",
    "\n",
    "In this lab we will use PyTorch. PyTorch is an open source deep learning framework primarily developed by Facebook's artificial-intelligence research group. In order to install PyTorch in your conda environment go to https://pytorch.org and select your operating system, conda, Python 3.6, no cuda. Copy the text from the \"Run this command:\" box. Now open a terminal and activate your 'ml2labs' conda environment. Paste the text and run. After the installation is done you should restart Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d9c3d77f550b5fd93b34fd18825c47f0",
     "grade": false,
     "grade_id": "cell-746cac8d9a21943b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### MNIST data\n",
    "\n",
    "In this Lab we will use several methods for unsupervised learning on the MNIST dataset of written digits. The dataset contains digital images of handwritten numbers $0$ through $9$. Each image has 28x28 pixels that each take 256 values in a range from white ($= 0$) to  black ($=1$). The labels belonging to the images are also included. \n",
    "Fortunately, PyTorch comes with a MNIST data loader. The first time you run the box below it will download the MNIST data set. That can take a couple of minutes.\n",
    "The main data types in PyTorch are tensors. For Part 1, we will convert those tensors to numpy arrays. In Part 2, we will use the torch module to directly work with PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fbc152afa1255331d7b88bf00b7156c",
     "grade": false,
     "grade_id": "cell-7c995be0fda080c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "train_labels = train_dataset.train_labels.numpy()\n",
    "train_data = train_dataset.train_data.numpy()\n",
    "# For EM we will use flattened data\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fc852f9bfb0bab10d4c23eada309e89",
     "grade": false,
     "grade_id": "cell-8b4a44df532b1867",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Expectation Maximization\n",
    "We will use the Expectation Maximization (EM) algorithm for the recognition of handwritten digits in the MNIST dataset. The images are modelled as a Bernoulli mixture model (see Bishop $\\S9.3.3$):\n",
    "$$\n",
    "p(\\bx|\\bmu, \\bpi) = \\sum_{k=1}^K  \\pi_k \\prod_{i=1}^D \\mu_{ki}^{x_i}(1-\\mu_{ki})^{(1-x_i)}\n",
    "$$\n",
    "where $x_i$ is the value of pixel $i$ in an image, $\\mu_{ki}$ represents the probability that pixel $i$ in class $k$ is black, and $\\{\\pi_1, \\ldots, \\pi_K\\}$ are the mixing coefficients of classes in the data. We want to use this data set to classify new images of handwritten numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54064637b7e7cf938c0f778d748a226a",
     "grade": false,
     "grade_id": "cell-af03fef663aa85b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Binary data (5 points)\n",
    "As we like to apply our Bernoulli mixture model, write a function `binarize` to convert the (flattened) MNIST data to binary images, where each pixel $x_i \\in \\{0,1\\}$, by thresholding at an appropriate level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fe8607a4d734f7f26ef1ee1e54b33471",
     "grade": false,
     "grade_id": "cell-ec4365531ca57ef3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def binarize(X):\n",
    "    return np.array(X > 128, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "231b2c9f29bc5c536c60cef4d74793a1",
     "grade": true,
     "grade_id": "cell-2f16f57cb68a83b3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test test test!\n",
    "bin_train_data = binarize(train_data)\n",
    "assert bin_train_data.dtype == np.float\n",
    "assert bin_train_data.shape == train_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a0a39404cc2f67078b399ee34653a3ac",
     "grade": false,
     "grade_id": "cell-462e747685e8670f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Sample a few images of digits $2$, $3$ and $4$; and show both the original and the binarized image together with their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f3c981f0fda5ba3bdfcefb9144305c7",
     "grade": true,
     "grade_id": "cell-784c6bd177a9aa42",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1149deb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a96d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1334f10f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1335cca20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1336adb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1337737b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133856278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133932780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAFmlJREFUeJzt3XmUXGWZx/HvL6GTQFgEw5qEsCSoMGcI2CwKnIkHkE0GmHNAEDAwalAB8QwqDKMSFUZQBBxZwxASZBNFgVGQAUY2USQgSNhkMZHsQIAEkKzP/HFvY9F1O11da9fbv885fbrqve+973Orn3r61t1KEYGZmbW/Qa0OwMzM6sMF3cwsES7oZmaJcEE3M0uEC7qZWSJc0M3MEuGCXkDSZZK+Ue++vSxnK0khaa06950gaU6VMVU9r9WfpFmS9ulh2l6Snm3g2LdLmljnZU6WdE0D+k6TdFaVMVU9b3/Qa0EYiCLi843omypJ44AngJ9FxDGtjmcgioj7gQ80cPkHNGrZ7UjSN4FvAftGxF2tjqeLt9C7kTS41TG0oYuBh1sdhNWfMq4TJSRtCxwOzG91LN0NiD+UpA9JukfS65KelPTPJdOmSbpU0m2S3gI+1v1jl6SvSZovaZ6kz+a7O8aWzH9W/niCpDmSTpW0KJ/n+JLlHCTpj5KWSHpJ0uQ6rd/xkp6WtFTSi5JOKOhzhqRX8o/tR5e0D5V0nqS/SlqY70Jauw9jHwm8Dtxdj3WxXu0i6SlJr0m6StIwKN89lv+dvyLpT5LekPSTkr4bSvqlpJfz5fxS0qiSee+RdLak3wJvA9vkbZ/Npz8u6c2Sn5A0IZ+2u6QH8/fa413t+bStJd2b5+mdwIhqXwRJP5W0IF+3+yTt0K3LCEl35mPdK2lMybwfzKctlvSspCP6OPzFwGnA8mrjb5TkC7qkDuB/gP8FNgFOBq6VVPrx9FPA2cB6wAPd5t8f+DdgH2AsMKGXITcDNgBGAp8BLpa0YT7tLeDTwPuAg4AvSDq02nUrsQj4BLA+cDxwgaSdu8U0Io9pIjClZP3PAbYDxpOt30jgm0WDSLpE0iUlz9cHvk32+lhzHA3sB2xL9nf7+hr6HgHsD2wN/CNwXN4+CLgKGANsCfwNuKjbvMcCk8jeE7NLJ0TEjhGxbkSsS/a3fxZ4VNJI4FfAWcBGwFeAmyRtnM96HfAIWS5+hywXq3U7MI7sPf0ocG236UfnY4wAHuuaLmk4cGceyybAkcAlkrYvGiT/x7RnyfPDgWURcVsNsTdORCT9A+wFLAAGlbRdD0zOH08Dru42zzTgrPzxVOC7JdPGAgGMLeg7gezNsVZJ/0XA7j3EdiFwQf54q3y5a1WwTmvsC9wMnFIS00pgeMn0G4FvACL7J7NtybSPAH8pmXfOGuL4IXBa/ngycE2r/94p/wCzgM+XPD8QeKHob5X3Pabk+feAy3pY7njgtZLn9wDf7tbnHuCz3dr2zPN7u/z5acCPu/W5g6xwb1mQh9dVmjNryi+yDaQANsifTwNuKJm+LrAKGA18Eri/2/yXA2eWzHtWD+OsBzwHbFXyGu/T6rwo/RkIB0W3AF6KiNUlbbPJtkS7vNTL/DMq7AvwakSsLHn+NllCIWk3si3ifwCGAEOBn/ayvF5JOgA4k2yLbRCwDtlByi6vRcRbJc9nk63XxnnfRyS9uzig1+MIksaTfWrZqdb4rU9K86/r79iTBSWP3+7qK2kd4AKyrfeuT4/rSRocEasKxikjaTTZhsHEiPhz3jwGOFzSwSVdO4Df5GMX5eHoNY3Tw9iDyT5RH06Ww13v7RHAG93jj4g3JS3OYxgD7Cbp9ZJFrgX8uIKhJ5P9w5rV15ibZSAU9HnAaEmDSor6lsCfS/qs6ZaT84FRJc/7nIAlriP7aHtARLwj6UJq2I8I2T5w4CayXTm3RMQKSTeTFeYuG0oaXvJm2hKYCbxC9olih4iY28ehJ5B9Uvhr/s9gXWCwpO0jYuc1zGe1Kc2/Lcnyu69OJTsjZreIWJD/c/4j782ZHt8T+TGWm4ELI+L2kkkvkRW8zxXMM4biPKzmdq+fAg4h26CYRbaL87Vu8b/7Oklal2wX0Lw8xnsjYt8qxt0bGCXpi/nzjYEbJZ0bEedWsby6S34fOvAQ2dbJ1yR15AdpDgZuqHD+G4HjlR1YXYdsV0W11gMW58V8V7LELKTsvNt7Klhm15b+y8DKfGv94wX9viVpiKS9yPa3/zT/B3cF2T73TfJxR0rar4Jxp5Dtxx2f/1xGtv+0knmteidKGiVpI+A/gJ9UsYz1yP6Rv54v58w+zj8VeCYivtet/RrgYEn7SRosaVh+sHZURMwm+6TblYd7kr0P35UfyD2uwviXAa+SfcL8z4I+B0raU9IQsn3pv4+Il4BfAttJOjavBx2SdpH0oQrG3Zvs03VXzs8DTiA7SNovJF/QI2I5WeIcQLZFegnw6Yh4psL5bwf+i+xj4/PA7/NJy6oI54vAtyUtJTvweOMa+o4GfltBfEuBL+XLeo3sn8St3botyKfNIzs49PmS9T+NfL0kLQHuoofzmZWdAXNZPu7bEbGg6wd4E3gnIl7uLWaryXVkB/hfBF4gOwDZVxcCa5O9H34P/LqP8x8JHNbtTJe98oJ5CHAG2QbGS8BX+Xud+RSwG7CY7J/I1V0LzAvv+/n7+2tNribbXTMXeKqHea7Lx1gMfBg4Bt59v3w8X4d5ZO+Nc8k2isp0rVs+76vdcn4V2W6kNyuIuSmU79y3CuX/yWcCQ7vtK6/3OI8Be0fEq40aw6y/yLfYT4yIo1odSztzQa+ApMOA28g+3k0HVkdEPU43NDOrm+R3udTJCWSnZ71A9jHrC60Nx8ysnLfQzcwS4S10M7NE1FTQJe2f3wvheUmn1ysos1Zzbls7qnqXS3611p+BfYE5ZHfbOyoinuppniEaGsMYXtV4Zr15h7dYHsvUe881c25bf1NpbtdypeiuwPMR8SKApBvIzkHtMemHMZzdtHcNQ5r17KGo2w0fndvWr1Sa27XschnJe+/3MIf33h8FAEmTJM2QNGNFVdfimDWdc9vaUsMPikbElIjojIjOjuKLsczaknPb+ptaCvpc3nujoFF5m1m7c25bW6qloD8MjFP2LSRDyO6N0P0eImbtyLltbanqg6IRsVLSSWQ3sB8MTI2IJ+sWmVmLOLetXdV0P/TIvoapf34Vk1kNnNvWjnylqJlZIlzQzcwS4YJuZpYIF3Qzs0S4oJuZJcIF3cwsES7oZmaJcEE3M0uEC7qZWSJc0M3MEuGCbmaWCBd0M7NEuKCbmSWiprstWv+nD+9Q1jbiR8Xf1fDaMe8ra1v54qx6h2TWMHfMe6ywfb8txjc5ktbwFrqZWSJc0M3MEuGCbmaWCBd0M7NE1HRQVNIsYCmwClgZEZ31CKong9+/UXEcG6xf1havLC7su2rJkrrG1N/NPniDsrZbxkwr7LvDF08uaxv77/MK+8aK5TXF1d81O7fN6qEeZ7l8LCJeqcNyzPob57a1Fe9yMTNLRK0FPYC7JD0iaVI9AjLrJ5zb1nZq3eWyZ0TMlbQJcKekZyLivtIO+ZthEsAw1qlxOLOmcW5b26lpCz0i5ua/FwG/AHYt6DMlIjojorODobUMZ9Y0zm1rR1VvoUsaDgyKiKX5448D365bZAWemTyusP3pf7morG3H/z6lsO+YMx+sa0z93SaPrCxv/Fxx3yeP+lFZ26HXTCzsG489VUtY/VorcrunS9aLDJTL2Oup6PVN8XWsZZfLpsAvJHUt57qI+HVdojJrLee2taWqC3pEvAjsWMdYzPoF57a1K5+2aGaWCBd0M7NEJHs/9FuP+35h+zF/+UpZ24bTftfocFrmrc0GtzoEq7OBfs9v65m30M3MEuGCbmaWCBd0M7NEuKCbmSXCBd3MLBHJnuUyZq0hhe1XTT6/rO1LL51U2Hetux+pa0yNNHjDDQvbP3rCjJqW+/xR5V+QAbBN5VeqW5MMlMvbu/TldgkDhbfQzcwS4YJuZpYIF3Qzs0S4oJuZJaKtDooOn137ZexjO8pXeejX5xf21ZOblrWtXLCw5hgaYfmOWxe2f3/zy5sciVWjp4OXtR74820CBhZvoZuZJcIF3cwsES7oZmaJcEE3M0tErwVd0lRJiyTNLGnbSNKdkp7LfxdfpmjWjzm3LTWVnOUyDbgIuLqk7XTg7og4R9Lp+fPT6h/ee4288A+F7TtsdnJZW9E32PfkF9vdUtjeeewpZW1bfL9/nuUyZM5rhe3XLx1Z1nbUenMrXu7Y698obF9d8RL6tWn0k9zuSdHZKL7k3XrS6xZ6RNwHLO7WfAgwPX88HTi0znGZNZxz21JT7T70TSOi6+TtBUD5Cdtm7cm5bW2r5oOiERFA9DRd0iRJMyTNWMGyWoczaxrntrWbagv6QkmbA+S/F/XUMSKmRERnRHR2MLTK4cyaxrltbavaS/9vBSYC5+S/i48q1lmsXFnYvt25L5S1TT9oTGHfievPrni8Tx77f2Vtv7tum8K+K+fOq3i5jbB8VPHJGH05AGpAi3K72QbavdMHikpOW7we+B3wAUlzJH2GLNn3lfQcsE/+3KytOLctNb1uoUfEUT1M2rvOsZg1lXPbUuMrRc3MEuGCbmaWCBd0M7NEtNUXXPRk1csvl7Wd/0TxbtCJe0yteLlfff8TZW2fGPuRwr6DajzLZdCwYWVts7+6c8Xz7/GJx2sa36y/8q0OKuctdDOzRLigm5klwgXdzCwRLuhmZolI4qBokY6H1yuesEdty5330bUL20fdW9627MBdCvvO/2j5y75yePk9oJ464od9C65Gl74+rqxt0MuvF/ZN5H7obamnS/Tb6eBhO8XaTryFbmaWCBd0M7NEuKCbmSXCBd3MLBHKvpSlOdbXRrGbWnsjuwU3f6isbcYu17Qgkvfq0OCythWxqgWRvNfOF5R/ATfAFuc92ORIevdQ3M2SWKxWjN0fctsHGvumne7/XmluewvdzCwRLuhmZolwQTczS4QLuplZIir5TtGpkhZJmlnSNlnSXEmP5T8HNjZMs/pzbltqKrn0fxpwEXB1t/YLIuK8ukfUYBtfWH7p/uprW38h+4qCk41W94ML7Jd1vtnqEBppGgnldtFZGz7zpWc9vTbtdPZLd71uoUfEfcDiJsRi1lTObUtNLfvQT5b0p/xj64Z1i8is9Zzb1paqLeiXAtsA44H5wA966ihpkqQZkmasYFmVw5k1jXPb2lZVBT0iFkbEqohYDVwB7LqGvlMiojMiOjsYWm2cZk3h3LZ2VtX90CVtHhHz86eHATPX1N96d/WSkWVtq3r4f/vdBw4qaxu8pPzWAQBPHvmj2gIbYJzb/U8K939vll4LuqTrgQnACElzgDOBCZLGAwHMAk5oYIxmDeHcttT0WtAj4qiC5isbEItZUzm3LTW+UtTMLBEu6GZmiXBBNzNLRFVnuVi5x5eXt936xs6FfX81Za+ytk0uqfwLI7bj4bK2VROKx+LIihdrVrN2vmw+Bd5CNzNLhAu6mVkiXNDNzBLhgm5mlogBd1B0yMzZZW3jH/zXwr67j55V1nb/C2ML+25zSfkNzfXb4kuTN6HyA6Ctdt6Hf1bYfvlm5Qd2Vy5Y2OhwrAV8oLN9eAvdzCwRLuhmZolwQTczS4QLuplZIlzQzcwSMeDOcln1yqtlbVseXt4GMK+gbVv+WOeI+rf91nmjsP3yYf6GnnbgM1QGFm+hm5klwgXdzCwRLuhmZolwQTczS0QlXxI9Grga2JTsi3OnRMQPJW0E/ATYiuzLdI+IiNcaF6qtSccrbxe23/u3dcra/mnt4r598cL3Nihr2+roIYV9Y0XBzeL7Aee2FbljXvktO9rl4HIlW+grgVMjYntgd+BESdsDpwN3R8Q44O78uVk7cW5bUnot6BExPyIezR8vBZ4GRgKHANPzbtOBQxsVpFkjOLctNX06D13SVsBOwEPAphExP5+0gOxja9E8k4BJAMMo//hv1h84ty0FFR8UlbQucBPw5YhYUjotIoJsH2SZiJgSEZ0R0dmBL0ax/se5bamoqKBL6iBL+Gsj4ud580JJm+fTNwcWNSZEs8ZxbltKKjnLRcCVwNMRcX7JpFuBicA5+e9bGhKhVWT1zGcK27974sSytsGXXFXYd89h71Q83uN7TC1rO2z4PoV9V73eb89ycW63saIzT4rOUBlIKtmHvgdwLPCEpK5X6wyyZL9R0meA2cARjQnRrGGc25aUXgt6RDwAqIfJe9c3HLPmcW5banylqJlZIlzQzcwSMeDuhz7QDLljRlnb2SccV9j3O1OuKGvrHLqq4rHenPCBwva1b/5Dxcswq0VPl+gPlIOl3kI3M0uEC7qZWSJc0M3MEuGCbmaWCBd0M7NE+CyXAajjrkcK20/6wUllbYdOuqew7/R79ypr++A9xbcfqPw8GbPGGCi3CfAWuplZIlzQzcwS4YJuZpYIF3Qzs0T4oKi9a5OLHyxre/DiIYV9x/FQWZsPflo76ek2Ae3MW+hmZolwQTczS4QLuplZIlzQzcwS0WtBlzRa0m8kPSXpSUmn5O2TJc2V9Fj+c2DjwzWrH+e2paaSs1xWAqdGxKOS1gMekXRnPu2CiDivceGZNZRz25JSyZdEzwfm54+XSnoaGNnowMwazbltqenTPnRJWwE7wbsnIZ8s6U+SpkrasId5JkmaIWnGCpbVFKxZozi3LQUVF3RJ6wI3AV+OiCXApcA2wHiyrZwfFM0XEVMiojMiOjsYWoeQzerLuW2pqKigS+ogS/hrI+LnABGxMCJWRcRq4Apg18aFadYYzm1LSSVnuQi4Eng6Is4vad+8pNthwMz6h2fWOM5tS00lZ7nsARwLPCGp647wZwBHSRoPBDALOKEhEZo1jnPbklLJWS4PACqYdFv9wzFrHue2pcZXipqZJcIF3cwsES7oZmaJcEE3M0uEC7qZWSJc0M3MEuGCbmaWCBd0M7NEKCKaN5j0MjA7fzoCeKVpgzeP16t1xkTExq0YuCS32+F1qlaq69YO61VRbje1oL9nYGlGRHS2ZPAG8noNbCm/TqmuW0rr5V0uZmaJcEE3M0tEKwv6lBaO3Uher4Et5dcp1XVLZr1atg/dzMzqy7tczMwS0fSCLml/Sc9Kel7S6c0ev57yLxBeJGlmSdtGku6U9Fz+u/ALhvszSaMl/UbSU5KelHRK3t7269ZIqeS287r91q1LUwu6pMHAxcABwPZk3wyzfTNjqLNpwP7d2k4H7o6IccDd+fN2sxI4NSK2B3YHTsz/TimsW0MkltvTcF63pWZvoe8KPB8RL0bEcuAG4JAmx1A3EXEfsLhb8yHA9PzxdODQpgZVBxExPyIezR8vBZ4GRpLAujVQMrntvG6/devS7II+Enip5PmcvC0lm0bE/PzxAmDTVgZTK0lbATsBD5HYutVZ6rmd1N8+1bz2QdEGiuwUorY9jUjSusBNwJcjYknptHZfN6teu//tU87rZhf0ucDokuej8raULJS0OUD+e1GL46mKpA6ypL82In6eNyexbg2Sem4n8bdPPa+bXdAfBsZJ2lrSEOBI4NYmx9BotwIT88cTgVtaGEtVJAm4Eng6Is4vmdT269ZAqed22//tB0JeN/3CIkkHAhcCg4GpEXF2UwOoI0nXAxPI7ta2EDgTuBm4EdiS7O57R0RE9wNM/ZqkPYH7gSeA1XnzGWT7G9t63Ropldx2XrffunXxlaJmZonwQVEzs0S4oJuZJcIF3cwsES7oZmaJcEE3M0uEC7qZWSJc0M3MEuGCbmaWiP8HsXMY4Eko9+EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133958438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "twos = np.argwhere(train_labels == 2)\n",
    "threes = np.argwhere(train_labels == 3)\n",
    "fours = np.argwhere(train_labels == 4)\n",
    "sample = np.concatenate((twos[:3], threes[:3], fours[:3]))\n",
    "\n",
    "for image_idx in sample:\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(train_data[image_idx].reshape(28, -1))\n",
    "    axes[0].set_title(\"original, label:\"+ str(train_labels[image_idx][0]))\n",
    "    axes[1].imshow(bin_train_data[image_idx].reshape(28, -1))\n",
    "    axes[1].set_title(\"binarized, label:\"+ str(train_labels[image_idx][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b9da574d24193df76e96ed8ca62c7b0",
     "grade": false,
     "grade_id": "cell-56b33654497d4052",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Implementation (40 points)\n",
    "You are going to write a function ```EM(X, K, max_iter)``` that implements the EM algorithm on the Bernoulli mixture model. \n",
    "\n",
    "The only parameters the function has are:\n",
    "* ```X``` :: (NxD) array of input training images\n",
    "* ```K``` :: size of the latent space\n",
    "* ```max_iter``` :: maximum number of iterations, i.e. one E-step and one M-step\n",
    "\n",
    "You are free to specify your return statement.\n",
    "\n",
    "Make sure you use a sensible way of terminating the iteration process early to prevent unnecessarily running through all epochs. Vectorize computations using ```numpy``` as  much as possible.\n",
    "\n",
    "You should implement the `E_step(X, mu, pi)` and `M_step(X, gamma)` separately in the functions defined below. These you can then use in your function `EM(X, K, max_iter)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "316c9131692747c363b5db8e9091d362",
     "grade": false,
     "grade_id": "cell-882b13c117a73cc4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "def E_step(X, mu, pi):\n",
    "    \n",
    "    # E-step\n",
    "#     print(\"Shape X\", X.shape)\n",
    "#     print(\"Shape mu\", mu.shape)\n",
    "#     print(\"Shape pi\", pi.shape)\n",
    "#     print(\"to obtain the right shape, Mu must be transposed\")\n",
    "    \n",
    "    # stabilizer to prevent underflow resulting in NaNs\n",
    "    stabilizer = 1e-3\n",
    "    \n",
    "    #Bishop 9.56\n",
    "    enumerator = np.dot(X, np.log(mu+stabilizer).T) + np.dot((1.0-X), np.log((1.0-mu.T+stabilizer))) + np.log(pi).T\n",
    "\n",
    "#     print('[debug] enumerator first part:', np.dot(X, np.log(mu+stabilizer).T))\n",
    "#     print('[debug] enumerator second part:', np.dot((1.0-X), np.log((1.0-mu.T+stabilizer))))\n",
    "#     print('[debug] enumerator first+second part:',np.dot(X, np.log(mu+stabilizer).T) + np.dot((1.0-X), np.log((1.0-mu.T+stabilizer))))\n",
    "#     print('[debug] enumerator third part:', np.log(pi).T)\n",
    "#     print('[debug] enumerator', enumerator)\n",
    "    #     print(\"shape Gamma: \", enumerator.shape)\n",
    "    # Note that Bishop 9.56 is for one znk, we want the whole matrix\n",
    "    # Sum of gamma can be obtained using the logsumexp (since we use ) (https://en.wikipedia.org/wiki/LogSumExp)\n",
    "    # This must be done over the columns (clusters) \n",
    "    denominator = logsumexp(enumerator, axis=1, keepdims=True)\n",
    "    \n",
    "    # exponent to be able to subtract two matrices\n",
    "    # EDIT: eerst van elkaar aftrekken in log-space en dan exponent\n",
    "    # want exp(log(x)) - exp(log(y)) = x - y != x/y\n",
    "#     print()\n",
    "#     print(\"[debug] enumerator\", enumerator)\n",
    "#     print(\"[debug] denominator\", denominator)\n",
    "#     print(\"[debug] enumerator-denominator\", enumerator-denominator)\n",
    "#     print()\n",
    "    gamma = np.exp(enumerator - denominator)\n",
    "    \n",
    "#     print(\"shape gamma_max: \",gamma.shape)\n",
    "        \n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_E_step(X, mu, pi):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    K = mu.shape[0]\n",
    "    D = X.shape[1]\n",
    "    \n",
    "    gamma = np.zeros((N,K))\n",
    "    \n",
    "    print(X, mu)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            gamma[n][k] = pi[k] * np.multiply.reduce(np.array([mu[k][i] ** X[n][i] * (1 - mu[k][i]) ** (1 - X[n][i]) for i in range(D)]))\n",
    "            \n",
    "            print('[debug] enumerator first+second part', np.multiply.reduce(np.array([mu[k][i] ** X[n][i] * (1 - mu[k][i]) ** (1 - X[n][i]) for i in range(D)])))\n",
    "        print(gamma[n, :])    \n",
    "        gamma[n, :] = gamma[n][k] / np.sum(gamma[n])\n",
    "            \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.97005988e-10,   9.96009978e-13,   9.97005988e-10,\n",
       "          9.96009978e-13,   9.99999998e-01],\n",
       "       [  9.97011952e-01,   9.96015936e-04,   9.93033855e-13,\n",
       "          9.96015936e-04,   9.96015936e-04],\n",
       "       [  9.98002997e-04,   9.99001000e-01,   9.94020945e-16,\n",
       "          9.97005991e-07,   9.95014966e-13],\n",
       "       [  4.98502995e-10,   4.98004990e-13,   4.98502995e-10,\n",
       "          5.00000000e-01,   5.00000000e-01],\n",
       "       [  4.99750374e-01,   4.99251123e-04,   4.99750374e-01,\n",
       "          4.98254116e-10,   4.98254116e-10],\n",
       "       [  4.99251870e-01,   4.98753117e-04,   4.99251870e-01,\n",
       "          4.98753117e-04,   4.98753117e-04],\n",
       "       [  9.98002001e-04,   9.97004996e-07,   9.96008987e-10,\n",
       "          9.97004996e-07,   9.99000003e-01],\n",
       "       [  9.97005989e-07,   9.96009979e-10,   9.99000998e-01,\n",
       "          9.98002995e-04,   9.96009979e-10],\n",
       "       [  9.97004993e-10,   9.96008984e-13,   9.97004993e-10,\n",
       "          9.99999000e-01,   9.98001998e-07],\n",
       "       [  9.97004994e-10,   9.96008985e-13,   9.95013971e-16,\n",
       "          9.98001999e-07,   9.99999001e-01],\n",
       "       [  9.97003998e-10,   9.98001002e-07,   9.97003998e-10,\n",
       "          9.98001002e-07,   9.99998002e-01],\n",
       "       [  9.97004994e-10,   9.98001999e-07,   9.93026924e-22,\n",
       "          9.96008985e-13,   9.99999001e-01],\n",
       "       [  9.97004993e-10,   9.96008984e-13,   9.97004993e-10,\n",
       "          9.99999000e-01,   9.98001998e-07],\n",
       "       [  9.97006987e-04,   9.96010976e-07,   9.97006987e-04,\n",
       "          9.98003994e-01,   9.96010976e-07],\n",
       "       [  9.97005990e-07,   9.96009980e-10,   9.99000999e-01,\n",
       "          9.98002996e-04,   9.94020944e-16],\n",
       "       [  9.98005981e-01,   9.97008972e-04,   9.94023917e-13,\n",
       "          9.95017941e-10,   9.97008972e-04],\n",
       "       [  4.99750374e-01,   4.98254116e-10,   4.99750374e-01,\n",
       "          4.99251123e-04,   4.98254116e-10],\n",
       "       [  9.97005989e-07,   9.96009979e-10,   9.99000998e-01,\n",
       "          9.98002995e-04,   9.96009979e-10],\n",
       "       [  9.94019954e-16,   9.97004997e-07,   9.98002002e-04,\n",
       "          9.99000004e-01,   9.97004997e-07],\n",
       "       [  9.97005989e-07,   9.96009979e-10,   9.99000998e-01,\n",
       "          9.98002995e-04,   9.96009979e-10]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_naive = np.random.uniform(size=(20, 10)) > 0.5 #np.array([[0, 0, 1], [0, 0, 0], [0, 1, 0], [1, 1, 1]])\n",
    "MU_naive = np.random.uniform(size=(5, 10)) > 0.5 * np.random.uniform(size=(5, 10)) + 1e-6 #np.array([[0.2, 0.1, 0.5], [0.1, 0.6, 0.4]])\n",
    "pi_naive = np.ones(5) / 5 #np.array([0.5, 0.5])\n",
    "\n",
    "E_step(X_naive.astype(np.float), MU_naive.astype(np.float), pi_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1418f4014e98024fc97446ce27766c1d",
     "grade": true,
     "grade_id": "cell-f7c7dd52d82e2498",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's test on 5 datapoints\n",
    "n_test = 5\n",
    "X_test = bin_train_data[:n_test]\n",
    "D_test, K_test = X_test.shape[1], 10\n",
    "\n",
    "np.random.seed(2018)\n",
    "mu_test = np.random.uniform(low=.25, high=.75, size=(K_test,D_test))\n",
    "pi_test = np.ones(K_test) / K_test\n",
    "\n",
    "gamma_test = E_step(X_test, mu_test, pi_test)\n",
    "assert gamma_test.shape == (n_test, K_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c426a613653174795cd9c8327ab6e20",
     "grade": false,
     "grade_id": "cell-f1b11b8765bd1ef6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    #keep dims to be able to divide the whole matrix at once for Pi\n",
    "    Nk_vec = gamma.sum(axis=0, keepdims= True) #gamma dims = (nxk), Nk dims= (1xk)\n",
    "    \n",
    "    # EDIT: N afleiden van data ipv in argument\n",
    "    N = X.shape[0]\n",
    "    K = gamma.shape[1]\n",
    "    D = X.shape[1]\n",
    "    \n",
    "#     print(\"Nk shape\", Nk_vec.shape)\n",
    "#     print(\"X shape, gamma shape\",X.shape,gamma.shape)\n",
    "    # TODO: Deze transposed enkel gedaan om de shape te laten kloppen\n",
    "    # niet logisch over nagedacht\n",
    "    # EDIT: Klopt, mu_k is proportioneel aan het dot product tussen\n",
    "    # de n'de rij van x en gamma. Ofwel x.T . gamma.\n",
    "    # EDIT: transpose genomen door X en gamma om te draaien\n",
    "    # EDIT: Hier ook verschil nemen in de exponent ipv tussen exponenten\n",
    "    mu = np.zeros((K, D))\n",
    "    for k in range(K):\n",
    "        mu[k, :] = gamma[:, k] @ X\n",
    "    mu = mu / Nk_vec.T\n",
    "    \n",
    "#     print(\"mu shape\", mu.shape)\n",
    "    pi = np.squeeze(Nk_vec/N)\n",
    "        \n",
    "    return mu, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_M_step(X, gamma):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    K = gamma.shape[1]\n",
    "    D = X.shape[1]\n",
    "    Nk = gamma.sum(axis=0)\n",
    "    \n",
    "    mu = np.zeros((K, D))\n",
    "    for k in range(K):\n",
    "        mu_k = np.zeros((D))\n",
    "        for n in range(N):\n",
    "            mu_k += gamma[n][k] * X[n, :]\n",
    "        mu[k, :] = mu_k / Nk[k]\n",
    "    \n",
    "    pi = Nk / N\n",
    "    \n",
    "    return mu, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.17996579e-10   5.45540625e-08   1.83252909e-07   1.21451619e-05\n",
      "   6.20953358e-06]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.45540625e-08   5.45540625e-08   5.45540625e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.45540625e-08\n",
      "   5.45540625e-08   5.50720591e-08   5.50720591e-08   1.21997160e-05\n",
      "   1.21456799e-05   1.83770906e-07   5.17996579e-10   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.17996579e-10\n",
      "   5.17996579e-10   5.17996579e-10   5.50720591e-08   5.50720591e-08\n",
      "   5.50720591e-08   5.50720591e-08   1.22002340e-05   1.22002340e-05\n",
      "   1.21456799e-05   1.83770906e-07   5.17996579e-10   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.83252909e-07   0.00000000e+00   0.00000000e+00\n",
      "   5.17996579e-10   5.17996579e-10   5.17996579e-10   5.17996579e-10\n",
      "   5.17996579e-10   6.26460564e-06   6.26460564e-06   6.26460564e-06\n",
      "   6.26460564e-06   5.50720591e-08   1.21451619e-05   1.84092496e-05\n",
      "   1.23829689e-05   1.83252909e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.83252909e-07   1.83252909e-07   0.00000000e+00   0.00000000e+00\n",
      "   5.17996579e-10   5.17996579e-10   5.17996579e-10   5.50720591e-08\n",
      "   6.26460564e-06   6.26460564e-06   6.26460564e-06   6.26460564e-06\n",
      "   6.26460564e-06   1.84097676e-05   1.83546955e-05   1.84092496e-05\n",
      "   6.44734055e-06   2.37806972e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.83252909e-07   1.83252909e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.17996579e-10   0.00000000e+00   6.26460564e-06\n",
      "   6.26460564e-06   6.26460564e-06   5.45540625e-08   0.00000000e+00\n",
      "   1.21997160e-05   1.84097676e-05   1.83546955e-05   6.39278649e-06\n",
      "   2.37806972e-07   5.45540625e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.83252909e-07   1.83252909e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   6.26408764e-06   6.26460564e-06\n",
      "   6.26460564e-06   5.45540625e-08   0.00000000e+00   1.21451619e-05\n",
      "   1.21451619e-05   1.83546955e-05   1.83546955e-05   6.39278649e-06\n",
      "   2.37806972e-07   5.45540625e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.83252909e-07   1.83252909e-07   0.00000000e+00   0.00000000e+00\n",
      "   6.20953358e-06   6.26408764e-06   6.26408764e-06   6.26460564e-06\n",
      "   5.50720591e-08   5.17996579e-10   0.00000000e+00   1.21451619e-05\n",
      "   1.83546955e-05   1.83546955e-05   6.20953358e-06   1.83252909e-07\n",
      "   2.37806972e-07   5.45540625e-08   5.45540625e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.83252909e-07\n",
      "   1.83252909e-07   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.26408764e-06   6.26408764e-06   6.26408764e-06   0.00000000e+00\n",
      "   5.17996579e-10   5.17996579e-10   1.21451619e-05   1.21451619e-05\n",
      "   1.83546955e-05   6.20953358e-06   6.39278649e-06   1.83252909e-07\n",
      "   2.37806972e-07   5.45540625e-08   5.45540625e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.83252909e-07\n",
      "   1.83252909e-07   0.00000000e+00   0.00000000e+00   6.20953358e-06\n",
      "   6.26408764e-06   6.26408764e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.17996579e-10   1.83552135e-05   1.83552135e-05\n",
      "   1.83546955e-05   6.39278649e-06   1.83252909e-07   1.83252909e-07\n",
      "   5.45540625e-08   5.45540625e-08   5.45540625e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.83252909e-07\n",
      "   1.83252909e-07   0.00000000e+00   0.00000000e+00   6.26408764e-06\n",
      "   6.26408764e-06   5.45540625e-08   0.00000000e+00   0.00000000e+00\n",
      "   6.39278649e-06   1.85379484e-05   1.85384664e-05   1.85384664e-05\n",
      "   6.39330448e-06   1.83252909e-07   1.83252909e-07   1.83252909e-07\n",
      "   5.45540625e-08   5.45540625e-08   5.45540625e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.83252909e-07\n",
      "   1.83252909e-07   1.83252909e-07   1.83252909e-07   6.44734055e-06\n",
      "   6.44734055e-06   6.39278649e-06   6.39278649e-06   6.39278649e-06\n",
      "   1.85379484e-05   1.85379484e-05   1.85379484e-05   1.83552135e-05\n",
      "   6.21005158e-06   5.17996579e-10   1.83770906e-07   1.83252909e-07\n",
      "   5.45540625e-08   5.45540625e-08   5.45540625e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.83252909e-07   1.83252909e-07   2.37806972e-07\n",
      "   6.44734055e-06   6.39278649e-06   6.20953358e-06   1.83546955e-05\n",
      "   1.21451619e-05   1.21451619e-05   1.83546955e-05   6.20953358e-06\n",
      "   6.20953358e-06   5.17996579e-10   1.83770906e-07   2.38324968e-07\n",
      "   5.45540625e-08   5.45540625e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.45540625e-08\n",
      "   5.45540625e-08   0.00000000e+00   0.00000000e+00   1.21451619e-05\n",
      "   1.21451619e-05   1.21451619e-05   1.83546955e-05   6.20953358e-06\n",
      "   0.00000000e+00   1.83770906e-07   2.38324968e-07   2.38324968e-07\n",
      "   5.45540625e-08   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.45540625e-08\n",
      "   5.45540625e-08   0.00000000e+00   1.21451619e-05   1.21451619e-05\n",
      "   1.21451619e-05   1.21451619e-05   6.20953358e-06   6.21005158e-06\n",
      "   5.17996579e-10   2.38324968e-07   2.38324968e-07   5.50720591e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.45540625e-08\n",
      "   5.45540625e-08   0.00000000e+00   1.21451619e-05   1.21451619e-05\n",
      "   1.21451619e-05   5.17996579e-10   6.21005158e-06   6.21005158e-06\n",
      "   5.50720591e-08   2.38324968e-07   2.38324968e-07   5.17996579e-10\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.45540625e-08\n",
      "   5.45540625e-08   1.21997160e-05   1.21451619e-05   1.21451619e-05\n",
      "   1.21456799e-05   5.50720591e-08   6.26460564e-06   6.26460564e-06\n",
      "   5.50720591e-08   2.38324968e-07   1.83252909e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.45540625e-08\n",
      "   5.45540625e-08   1.21997160e-05   1.22002340e-05   1.22002340e-05\n",
      "   1.22002340e-05   5.50720591e-08   6.26460564e-06   6.26460564e-06\n",
      "   5.45540625e-08   1.83252909e-07   1.83252909e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.50720591e-08\n",
      "   5.50720591e-08   1.22002340e-05   1.22002340e-05   1.22002340e-05\n",
      "   5.50720591e-08   5.50720591e-08   6.26408764e-06   6.20953358e-06\n",
      "   0.00000000e+00   1.83252909e-07   1.83252909e-07   1.83252909e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.17996579e-10   5.17996579e-10   5.17996579e-10\n",
      "   5.17996579e-10   1.22002340e-05   1.22002340e-05   1.22002340e-05\n",
      "   5.50720591e-08   0.00000000e+00   6.20953358e-06   6.20953358e-06\n",
      "   0.00000000e+00   1.83252909e-07   1.83252909e-07   1.83252909e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.17996579e-10   5.17996579e-10   5.17996579e-10   5.17996579e-10\n",
      "   5.17996579e-10   1.21456799e-05   1.21456799e-05   1.21451619e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   6.20953358e-06\n",
      "   6.20953358e-06   0.00000000e+00   1.83252909e-07   1.83252909e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   6.20953358e-06\n",
      "   6.20953358e-06   6.20953358e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.20953358e-06   6.20953358e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  1.55081445e-09   9.63739819e-05   1.53319126e-07   2.91308864e-12\n",
      "   2.35087578e-09]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.63739819e-05   9.63739819e-05   9.63739819e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.63739819e-05\n",
      "   9.63739819e-05   9.63755327e-05   9.63755327e-05   9.63739848e-05\n",
      "   1.55372754e-09   1.54869940e-07   1.55081445e-09   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.55081445e-09\n",
      "   1.55081445e-09   1.55081445e-09   9.63755327e-05   9.63755327e-05\n",
      "   9.63755327e-05   9.63755327e-05   9.63755356e-05   9.63755356e-05\n",
      "   1.55372754e-09   1.54869940e-07   1.55081445e-09   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.53319126e-07   0.00000000e+00   0.00000000e+00\n",
      "   1.55081445e-09   1.55081445e-09   1.55081445e-09   1.55081445e-09\n",
      "   1.55081445e-09   9.63778835e-05   9.63778835e-05   9.63778835e-05\n",
      "   9.63778835e-05   9.63755327e-05   2.91308864e-12   9.63763356e-05\n",
      "   9.65273039e-05   1.53319126e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.53319126e-07   1.53319126e-07   0.00000000e+00   0.00000000e+00\n",
      "   1.55081445e-09   1.55081445e-09   1.55081445e-09   9.63755327e-05\n",
      "   9.63778835e-05   9.63778835e-05   9.63778835e-05   9.63778835e-05\n",
      "   9.63778835e-05   9.63778865e-05   2.35378887e-09   9.63763356e-05\n",
      "   9.65296519e-05   9.65273010e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.53319126e-07   1.53319126e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.55081445e-09   0.00000000e+00   9.63778835e-05\n",
      "   9.63778835e-05   9.63778835e-05   9.63739819e-05   0.00000000e+00\n",
      "   9.63739848e-05   9.63778865e-05   2.35378887e-09   1.55670002e-07\n",
      "   9.65273010e-05   9.63739819e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.53319126e-07   1.53319126e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   9.63763327e-05   9.63778835e-05\n",
      "   9.63778835e-05   9.63739819e-05   0.00000000e+00   2.91308864e-12\n",
      "   2.91308864e-12   2.35378887e-09   2.35378887e-09   1.55670002e-07\n",
      "   9.65273010e-05   9.63739819e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.53319126e-07   1.53319126e-07   0.00000000e+00   0.00000000e+00\n",
      "   2.35087578e-09   9.63763327e-05   9.63763327e-05   9.63778835e-05\n",
      "   9.63755327e-05   1.55081445e-09   0.00000000e+00   2.91308864e-12\n",
      "   2.35378887e-09   2.35378887e-09   2.35087578e-09   1.53319126e-07\n",
      "   9.65273010e-05   9.63739819e-05   9.63739819e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.53319126e-07\n",
      "   1.53319126e-07   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.63763327e-05   9.63763327e-05   9.63763327e-05   0.00000000e+00\n",
      "   1.55081445e-09   1.55081445e-09   2.91308864e-12   2.91308864e-12\n",
      "   2.35378887e-09   2.35087578e-09   1.55670002e-07   1.53319126e-07\n",
      "   9.65273010e-05   9.63739819e-05   9.63739819e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.53319126e-07\n",
      "   1.53319126e-07   0.00000000e+00   0.00000000e+00   2.35087578e-09\n",
      "   9.63763327e-05   9.63763327e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.55081445e-09   3.90460332e-09   3.90460332e-09\n",
      "   2.35378887e-09   1.55670002e-07   1.53319126e-07   1.53319126e-07\n",
      "   9.63739819e-05   9.63739819e-05   9.63739819e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.53319126e-07\n",
      "   1.53319126e-07   0.00000000e+00   0.00000000e+00   9.63763327e-05\n",
      "   9.63763327e-05   9.63739819e-05   0.00000000e+00   0.00000000e+00\n",
      "   1.55670002e-07   1.55672915e-07   1.57223729e-07   1.57223729e-07\n",
      "   1.57220816e-07   1.53319126e-07   1.53319126e-07   1.53319126e-07\n",
      "   9.63739819e-05   9.63739819e-05   9.63739819e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.53319126e-07\n",
      "   1.53319126e-07   1.53319126e-07   1.53319126e-07   9.65296519e-05\n",
      "   9.65296519e-05   1.55670002e-07   1.55670002e-07   1.55670002e-07\n",
      "   1.55672915e-07   1.55672915e-07   1.55672915e-07   3.90460332e-09\n",
      "   3.90169023e-09   1.55081445e-09   1.54869940e-07   1.53319126e-07\n",
      "   9.63739819e-05   9.63739819e-05   9.63739819e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.53319126e-07   1.53319126e-07   9.65273010e-05\n",
      "   9.65296519e-05   1.55670002e-07   2.35087578e-09   2.35378887e-09\n",
      "   2.91308864e-12   2.91308864e-12   2.35378887e-09   2.35087578e-09\n",
      "   2.35087578e-09   1.55081445e-09   1.54869940e-07   9.65288518e-05\n",
      "   9.63739819e-05   9.63739819e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.63739819e-05\n",
      "   9.63739819e-05   0.00000000e+00   0.00000000e+00   2.91308864e-12\n",
      "   2.91308864e-12   2.91308864e-12   2.35378887e-09   2.35087578e-09\n",
      "   0.00000000e+00   1.54869940e-07   9.65288518e-05   9.65288518e-05\n",
      "   9.63739819e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.63739819e-05\n",
      "   9.63739819e-05   0.00000000e+00   2.91308864e-12   2.91308864e-12\n",
      "   2.91308864e-12   2.91308864e-12   2.35087578e-09   3.90169023e-09\n",
      "   1.55081445e-09   9.65288518e-05   9.65288518e-05   9.63755327e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.63739819e-05\n",
      "   9.63739819e-05   0.00000000e+00   2.91308864e-12   2.91308864e-12\n",
      "   2.91308864e-12   1.55081445e-09   3.90169023e-09   3.90169023e-09\n",
      "   9.63755327e-05   9.65288518e-05   9.65288518e-05   1.55081445e-09\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.63739819e-05\n",
      "   9.63739819e-05   9.63739848e-05   2.91308864e-12   2.91308864e-12\n",
      "   1.55372754e-09   9.63755327e-05   9.63778835e-05   9.63778835e-05\n",
      "   9.63755327e-05   9.65288518e-05   1.53319126e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.63739819e-05\n",
      "   9.63739819e-05   9.63739848e-05   9.63755356e-05   9.63755356e-05\n",
      "   9.63755356e-05   9.63755327e-05   9.63778835e-05   9.63778835e-05\n",
      "   9.63739819e-05   1.53319126e-07   1.53319126e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.63755327e-05\n",
      "   9.63755327e-05   9.63755356e-05   9.63755356e-05   9.63755356e-05\n",
      "   9.63755327e-05   9.63755327e-05   9.63763327e-05   2.35087578e-09\n",
      "   0.00000000e+00   1.53319126e-07   1.53319126e-07   1.53319126e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.55081445e-09   1.55081445e-09   1.55081445e-09\n",
      "   1.55081445e-09   9.63755356e-05   9.63755356e-05   9.63755356e-05\n",
      "   9.63755327e-05   0.00000000e+00   2.35087578e-09   2.35087578e-09\n",
      "   0.00000000e+00   1.53319126e-07   1.53319126e-07   1.53319126e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.55081445e-09   1.55081445e-09   1.55081445e-09   1.55081445e-09\n",
      "   1.55081445e-09   1.55372754e-09   1.55372754e-09   2.91308864e-12\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.35087578e-09\n",
      "   2.35087578e-09   0.00000000e+00   1.53319126e-07   1.53319126e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.35087578e-09\n",
      "   2.35087578e-09   2.35087578e-09   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.35087578e-09   2.35087578e-09   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  3.03683581e-11   3.11084623e-07   8.13053416e-06   5.27787590e-08\n",
      "   5.10607918e-07]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.11084623e-07   3.11084623e-07   3.11084623e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.11084623e-07\n",
      "   3.11084623e-07   3.11114991e-07   3.11114991e-07   3.63863382e-07\n",
      "   5.28091273e-08   8.13056453e-06   3.03683581e-11   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.03683581e-11\n",
      "   3.03683581e-11   3.03683581e-11   3.11114991e-07   3.11114991e-07\n",
      "   3.11114991e-07   3.11114991e-07   3.63893750e-07   3.63893750e-07\n",
      "   5.28091273e-08   8.13056453e-06   3.03683581e-11   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.13053416e-06   0.00000000e+00   0.00000000e+00\n",
      "   3.03683581e-11   3.03683581e-11   3.03683581e-11   3.03683581e-11\n",
      "   3.03683581e-11   8.21722909e-07   8.21722909e-07   8.21722909e-07\n",
      "   8.21722909e-07   3.11114991e-07   5.27787590e-08   8.74471300e-07\n",
      "   8.49439754e-06   8.13053416e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.13053416e-06   8.13053416e-06   0.00000000e+00   0.00000000e+00\n",
      "   3.03683581e-11   3.03683581e-11   3.03683581e-11   3.11114991e-07\n",
      "   8.21722909e-07   8.21722909e-07   8.21722909e-07   8.21722909e-07\n",
      "   8.21722909e-07   8.74501668e-07   5.63386677e-07   8.74471300e-07\n",
      "   8.95222670e-06   8.44161878e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.13053416e-06   8.13053416e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.03683581e-11   0.00000000e+00   8.21722909e-07\n",
      "   8.21722909e-07   8.21722909e-07   3.11084623e-07   0.00000000e+00\n",
      "   3.63863382e-07   8.74501668e-07   5.63386677e-07   8.64114208e-06\n",
      "   8.44161878e-06   3.11084623e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.13053416e-06   8.13053416e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   8.21692541e-07   8.21722909e-07\n",
      "   8.21722909e-07   3.11084623e-07   0.00000000e+00   5.27787590e-08\n",
      "   5.27787590e-08   5.63386677e-07   5.63386677e-07   8.64114208e-06\n",
      "   8.44161878e-06   3.11084623e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.13053416e-06   8.13053416e-06   0.00000000e+00   0.00000000e+00\n",
      "   5.10607918e-07   8.21692541e-07   8.21692541e-07   8.21722909e-07\n",
      "   3.11114991e-07   3.03683581e-11   0.00000000e+00   5.27787590e-08\n",
      "   5.63386677e-07   5.63386677e-07   5.10607918e-07   8.13053416e-06\n",
      "   8.44161878e-06   3.11084623e-07   3.11084623e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.13053416e-06\n",
      "   8.13053416e-06   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.21692541e-07   8.21692541e-07   8.21692541e-07   0.00000000e+00\n",
      "   3.03683581e-11   3.03683581e-11   5.27787590e-08   5.27787590e-08\n",
      "   5.63386677e-07   5.10607918e-07   8.64114208e-06   8.13053416e-06\n",
      "   8.44161878e-06   3.11084623e-07   3.11084623e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.13053416e-06\n",
      "   8.13053416e-06   0.00000000e+00   0.00000000e+00   5.10607918e-07\n",
      "   8.21692541e-07   8.21692541e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.03683581e-11   5.63417045e-07   5.63417045e-07\n",
      "   5.63386677e-07   8.64114208e-06   8.13053416e-06   8.13053416e-06\n",
      "   3.11084623e-07   3.11084623e-07   3.11084623e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.13053416e-06\n",
      "   8.13053416e-06   0.00000000e+00   0.00000000e+00   8.21692541e-07\n",
      "   8.21692541e-07   3.11084623e-07   0.00000000e+00   0.00000000e+00\n",
      "   8.64114208e-06   8.69392083e-06   8.69395120e-06   8.69395120e-06\n",
      "   8.64117244e-06   8.13053416e-06   8.13053416e-06   8.13053416e-06\n",
      "   3.11084623e-07   3.11084623e-07   3.11084623e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.13053416e-06\n",
      "   8.13053416e-06   8.13053416e-06   8.13053416e-06   8.95222670e-06\n",
      "   8.95222670e-06   8.64114208e-06   8.64114208e-06   8.64114208e-06\n",
      "   8.69392083e-06   8.69392083e-06   8.69392083e-06   5.63417045e-07\n",
      "   5.10638286e-07   3.03683581e-11   8.13056453e-06   8.13053416e-06\n",
      "   3.11084623e-07   3.11084623e-07   3.11084623e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.13053416e-06   8.13053416e-06   8.44161878e-06\n",
      "   8.95222670e-06   8.64114208e-06   5.10607918e-07   5.63386677e-07\n",
      "   5.27787590e-08   5.27787590e-08   5.63386677e-07   5.10607918e-07\n",
      "   5.10607918e-07   3.03683581e-11   8.13056453e-06   8.44164915e-06\n",
      "   3.11084623e-07   3.11084623e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.11084623e-07\n",
      "   3.11084623e-07   0.00000000e+00   0.00000000e+00   5.27787590e-08\n",
      "   5.27787590e-08   5.27787590e-08   5.63386677e-07   5.10607918e-07\n",
      "   0.00000000e+00   8.13056453e-06   8.44164915e-06   8.44164915e-06\n",
      "   3.11084623e-07   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.11084623e-07\n",
      "   3.11084623e-07   0.00000000e+00   5.27787590e-08   5.27787590e-08\n",
      "   5.27787590e-08   5.27787590e-08   5.10607918e-07   5.10638286e-07\n",
      "   3.03683581e-11   8.44164915e-06   8.44164915e-06   3.11114991e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.11084623e-07\n",
      "   3.11084623e-07   0.00000000e+00   5.27787590e-08   5.27787590e-08\n",
      "   5.27787590e-08   3.03683581e-11   5.10638286e-07   5.10638286e-07\n",
      "   3.11114991e-07   8.44164915e-06   8.44164915e-06   3.03683581e-11\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.11084623e-07\n",
      "   3.11084623e-07   3.63863382e-07   5.27787590e-08   5.27787590e-08\n",
      "   5.28091273e-08   3.11114991e-07   8.21722909e-07   8.21722909e-07\n",
      "   3.11114991e-07   8.44164915e-06   8.13053416e-06   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.11084623e-07\n",
      "   3.11084623e-07   3.63863382e-07   3.63893750e-07   3.63893750e-07\n",
      "   3.63893750e-07   3.11114991e-07   8.21722909e-07   8.21722909e-07\n",
      "   3.11084623e-07   8.13053416e-06   8.13053416e-06   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.11114991e-07\n",
      "   3.11114991e-07   3.63893750e-07   3.63893750e-07   3.63893750e-07\n",
      "   3.11114991e-07   3.11114991e-07   8.21692541e-07   5.10607918e-07\n",
      "   0.00000000e+00   8.13053416e-06   8.13053416e-06   8.13053416e-06\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.03683581e-11   3.03683581e-11   3.03683581e-11\n",
      "   3.03683581e-11   3.63893750e-07   3.63893750e-07   3.63893750e-07\n",
      "   3.11114991e-07   0.00000000e+00   5.10607918e-07   5.10607918e-07\n",
      "   0.00000000e+00   8.13053416e-06   8.13053416e-06   8.13053416e-06\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.03683581e-11   3.03683581e-11   3.03683581e-11   3.03683581e-11\n",
      "   3.03683581e-11   5.28091273e-08   5.28091273e-08   5.27787590e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.10607918e-07\n",
      "   5.10607918e-07   0.00000000e+00   8.13053416e-06   8.13053416e-06\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.10607918e-07\n",
      "   5.10607918e-07   5.10607918e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.10607918e-07   5.10607918e-07   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  2.19248544e-09   1.21063826e-04   4.10112529e-10   2.41301703e-09\n",
      "   5.25480428e-12]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.21063826e-04   1.21063826e-04   1.21063826e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.21063826e-04\n",
      "   1.21063826e-04   1.21066018e-04   1.21066018e-04   1.21066239e-04\n",
      "   4.60550248e-09   2.60259797e-09   2.19248544e-09   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.19248544e-09\n",
      "   2.19248544e-09   2.19248544e-09   1.21066018e-04   1.21066018e-04\n",
      "   1.21066018e-04   1.21066018e-04   1.21068431e-04   1.21068431e-04\n",
      "   4.60550248e-09   2.60259797e-09   2.19248544e-09   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   4.10112529e-10   0.00000000e+00   0.00000000e+00\n",
      "   2.19248544e-09   2.19248544e-09   2.19248544e-09   2.19248544e-09\n",
      "   2.19248544e-09   1.21066024e-04   1.21066024e-04   1.21066024e-04\n",
      "   1.21066024e-04   1.21066018e-04   2.41301703e-09   1.21066244e-04\n",
      "   1.21066649e-04   4.10112529e-10   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.10112529e-10   4.10112529e-10   0.00000000e+00   0.00000000e+00\n",
      "   2.19248544e-09   2.19248544e-09   2.19248544e-09   1.21066018e-04\n",
      "   1.21066024e-04   1.21066024e-04   1.21066024e-04   1.21066024e-04\n",
      "   1.21066024e-04   1.21068437e-04   2.41827184e-09   1.21066244e-04\n",
      "   1.21064241e-04   1.21064236e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.10112529e-10   4.10112529e-10   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.19248544e-09   0.00000000e+00   1.21066024e-04\n",
      "   1.21066024e-04   1.21066024e-04   1.21063826e-04   0.00000000e+00\n",
      "   1.21066239e-04   1.21068437e-04   2.41827184e-09   4.15367333e-10\n",
      "   1.21064236e-04   1.21063826e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.10112529e-10   4.10112529e-10   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.21063831e-04   1.21066024e-04\n",
      "   1.21066024e-04   1.21063826e-04   0.00000000e+00   2.41301703e-09\n",
      "   2.41301703e-09   2.41827184e-09   2.41827184e-09   4.15367333e-10\n",
      "   1.21064236e-04   1.21063826e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.10112529e-10   4.10112529e-10   0.00000000e+00   0.00000000e+00\n",
      "   5.25480428e-12   1.21063831e-04   1.21063831e-04   1.21066024e-04\n",
      "   1.21066018e-04   2.19248544e-09   0.00000000e+00   2.41301703e-09\n",
      "   2.41827184e-09   2.41827184e-09   5.25480428e-12   4.10112529e-10\n",
      "   1.21064236e-04   1.21063826e-04   1.21063826e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.10112529e-10\n",
      "   4.10112529e-10   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.21063831e-04   1.21063831e-04   1.21063831e-04   0.00000000e+00\n",
      "   2.19248544e-09   2.19248544e-09   2.41301703e-09   2.41301703e-09\n",
      "   2.41827184e-09   5.25480428e-12   4.15367333e-10   4.10112529e-10\n",
      "   1.21064236e-04   1.21063826e-04   1.21063826e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.10112529e-10\n",
      "   4.10112529e-10   0.00000000e+00   0.00000000e+00   5.25480428e-12\n",
      "   1.21063831e-04   1.21063831e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.19248544e-09   4.61075728e-09   4.61075728e-09\n",
      "   2.41827184e-09   4.15367333e-10   4.10112529e-10   4.10112529e-10\n",
      "   1.21063826e-04   1.21063826e-04   1.21063826e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.10112529e-10\n",
      "   4.10112529e-10   0.00000000e+00   0.00000000e+00   1.21063831e-04\n",
      "   1.21063831e-04   1.21063826e-04   0.00000000e+00   0.00000000e+00\n",
      "   4.15367333e-10   2.82838437e-09   5.02086981e-09   5.02086981e-09\n",
      "   2.60785278e-09   4.10112529e-10   4.10112529e-10   4.10112529e-10\n",
      "   1.21063826e-04   1.21063826e-04   1.21063826e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.10112529e-10\n",
      "   4.10112529e-10   4.10112529e-10   4.10112529e-10   1.21064241e-04\n",
      "   1.21064241e-04   4.15367333e-10   4.15367333e-10   4.15367333e-10\n",
      "   2.82838437e-09   2.82838437e-09   2.82838437e-09   4.61075728e-09\n",
      "   2.19774025e-09   2.19248544e-09   2.60259797e-09   4.10112529e-10\n",
      "   1.21063826e-04   1.21063826e-04   1.21063826e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   4.10112529e-10   4.10112529e-10   1.21064236e-04\n",
      "   1.21064241e-04   4.15367333e-10   5.25480428e-12   2.41827184e-09\n",
      "   2.41301703e-09   2.41301703e-09   2.41827184e-09   5.25480428e-12\n",
      "   5.25480428e-12   2.19248544e-09   2.60259797e-09   1.21066428e-04\n",
      "   1.21063826e-04   1.21063826e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.21063826e-04\n",
      "   1.21063826e-04   0.00000000e+00   0.00000000e+00   2.41301703e-09\n",
      "   2.41301703e-09   2.41301703e-09   2.41827184e-09   5.25480428e-12\n",
      "   0.00000000e+00   2.60259797e-09   1.21066428e-04   1.21066428e-04\n",
      "   1.21063826e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.21063826e-04\n",
      "   1.21063826e-04   0.00000000e+00   2.41301703e-09   2.41301703e-09\n",
      "   2.41301703e-09   2.41301703e-09   5.25480428e-12   2.19774025e-09\n",
      "   2.19248544e-09   1.21066428e-04   1.21066428e-04   1.21066018e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.21063826e-04\n",
      "   1.21063826e-04   0.00000000e+00   2.41301703e-09   2.41301703e-09\n",
      "   2.41301703e-09   2.19248544e-09   2.19774025e-09   2.19774025e-09\n",
      "   1.21066018e-04   1.21066428e-04   1.21066428e-04   2.19248544e-09\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.21063826e-04\n",
      "   1.21063826e-04   1.21066239e-04   2.41301703e-09   2.41301703e-09\n",
      "   4.60550248e-09   1.21066018e-04   1.21066024e-04   1.21066024e-04\n",
      "   1.21066018e-04   1.21066428e-04   4.10112529e-10   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.21063826e-04\n",
      "   1.21063826e-04   1.21066239e-04   1.21068431e-04   1.21068431e-04\n",
      "   1.21068431e-04   1.21066018e-04   1.21066024e-04   1.21066024e-04\n",
      "   1.21063826e-04   4.10112529e-10   4.10112529e-10   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.21066018e-04\n",
      "   1.21066018e-04   1.21068431e-04   1.21068431e-04   1.21068431e-04\n",
      "   1.21066018e-04   1.21066018e-04   1.21063831e-04   5.25480428e-12\n",
      "   0.00000000e+00   4.10112529e-10   4.10112529e-10   4.10112529e-10\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.19248544e-09   2.19248544e-09   2.19248544e-09\n",
      "   2.19248544e-09   1.21068431e-04   1.21068431e-04   1.21068431e-04\n",
      "   1.21066018e-04   0.00000000e+00   5.25480428e-12   5.25480428e-12\n",
      "   0.00000000e+00   4.10112529e-10   4.10112529e-10   4.10112529e-10\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.19248544e-09   2.19248544e-09   2.19248544e-09   2.19248544e-09\n",
      "   2.19248544e-09   4.60550248e-09   4.60550248e-09   2.41301703e-09\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.25480428e-12\n",
      "   5.25480428e-12   0.00000000e+00   4.10112529e-10   4.10112529e-10\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.25480428e-12\n",
      "   5.25480428e-12   5.25480428e-12   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.25480428e-12   5.25480428e-12   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  2.17700959e-04   4.69208753e-01   1.47308788e-01   9.86345729e-01\n",
      "   7.64512370e-01]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.69208753e-01   4.69208753e-01   4.69208753e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.69208753e-01\n",
      "   4.69208753e-01   4.69426454e-01   4.69426454e-01   1.45555448e+00\n",
      "   9.86563430e-01   1.47526488e-01   2.17700959e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.17700959e-04\n",
      "   2.17700959e-04   2.17700959e-04   4.69426454e-01   4.69426454e-01\n",
      "   4.69426454e-01   4.69426454e-01   1.45577218e+00   1.45577218e+00\n",
      "   9.86563430e-01   1.47526488e-01   2.17700959e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.47308788e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.17700959e-04   2.17700959e-04   2.17700959e-04   2.17700959e-04\n",
      "   2.17700959e-04   1.23393882e+00   1.23393882e+00   1.23393882e+00\n",
      "   1.23393882e+00   4.69426454e-01   9.86345729e-01   2.22006685e+00\n",
      "   1.60286327e+00   1.47308788e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.47308788e-01   1.47308788e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.17700959e-04   2.17700959e-04   2.17700959e-04   4.69426454e-01\n",
      "   1.23393882e+00   1.23393882e+00   1.23393882e+00   1.23393882e+00\n",
      "   1.23393882e+00   2.22028455e+00   1.75085810e+00   2.22006685e+00\n",
      "   1.38102991e+00   6.16517541e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.47308788e-01   1.47308788e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.17700959e-04   0.00000000e+00   1.23393882e+00\n",
      "   1.23393882e+00   1.23393882e+00   4.69208753e-01   0.00000000e+00\n",
      "   1.45555448e+00   2.22028455e+00   1.75085810e+00   9.11821157e-01\n",
      "   6.16517541e-01   4.69208753e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.47308788e-01   1.47308788e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.23372112e+00   1.23393882e+00\n",
      "   1.23393882e+00   4.69208753e-01   0.00000000e+00   9.86345729e-01\n",
      "   9.86345729e-01   1.75085810e+00   1.75085810e+00   9.11821157e-01\n",
      "   6.16517541e-01   4.69208753e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.47308788e-01   1.47308788e-01   0.00000000e+00   0.00000000e+00\n",
      "   7.64512370e-01   1.23372112e+00   1.23372112e+00   1.23393882e+00\n",
      "   4.69426454e-01   2.17700959e-04   0.00000000e+00   9.86345729e-01\n",
      "   1.75085810e+00   1.75085810e+00   7.64512370e-01   1.47308788e-01\n",
      "   6.16517541e-01   4.69208753e-01   4.69208753e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.47308788e-01\n",
      "   1.47308788e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.23372112e+00   1.23372112e+00   1.23372112e+00   0.00000000e+00\n",
      "   2.17700959e-04   2.17700959e-04   9.86345729e-01   9.86345729e-01\n",
      "   1.75085810e+00   7.64512370e-01   9.11821157e-01   1.47308788e-01\n",
      "   6.16517541e-01   4.69208753e-01   4.69208753e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.47308788e-01\n",
      "   1.47308788e-01   0.00000000e+00   0.00000000e+00   7.64512370e-01\n",
      "   1.23372112e+00   1.23372112e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.17700959e-04   1.75107580e+00   1.75107580e+00\n",
      "   1.75085810e+00   9.11821157e-01   1.47308788e-01   1.47308788e-01\n",
      "   4.69208753e-01   4.69208753e-01   4.69208753e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.47308788e-01\n",
      "   1.47308788e-01   0.00000000e+00   0.00000000e+00   1.23372112e+00\n",
      "   1.23372112e+00   4.69208753e-01   0.00000000e+00   0.00000000e+00\n",
      "   9.11821157e-01   1.89816689e+00   1.89838459e+00   1.89838459e+00\n",
      "   9.12038858e-01   1.47308788e-01   1.47308788e-01   1.47308788e-01\n",
      "   4.69208753e-01   4.69208753e-01   4.69208753e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.47308788e-01\n",
      "   1.47308788e-01   1.47308788e-01   1.47308788e-01   1.38102991e+00\n",
      "   1.38102991e+00   9.11821157e-01   9.11821157e-01   9.11821157e-01\n",
      "   1.89816689e+00   1.89816689e+00   1.89816689e+00   1.75107580e+00\n",
      "   7.64730071e-01   2.17700959e-04   1.47526488e-01   1.47308788e-01\n",
      "   4.69208753e-01   4.69208753e-01   4.69208753e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.47308788e-01   1.47308788e-01   6.16517541e-01\n",
      "   1.38102991e+00   9.11821157e-01   7.64512370e-01   1.75085810e+00\n",
      "   9.86345729e-01   9.86345729e-01   1.75085810e+00   7.64512370e-01\n",
      "   7.64512370e-01   2.17700959e-04   1.47526488e-01   6.16735242e-01\n",
      "   4.69208753e-01   4.69208753e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.69208753e-01\n",
      "   4.69208753e-01   0.00000000e+00   0.00000000e+00   9.86345729e-01\n",
      "   9.86345729e-01   9.86345729e-01   1.75085810e+00   7.64512370e-01\n",
      "   0.00000000e+00   1.47526488e-01   6.16735242e-01   6.16735242e-01\n",
      "   4.69208753e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.69208753e-01\n",
      "   4.69208753e-01   0.00000000e+00   9.86345729e-01   9.86345729e-01\n",
      "   9.86345729e-01   9.86345729e-01   7.64512370e-01   7.64730071e-01\n",
      "   2.17700959e-04   6.16735242e-01   6.16735242e-01   4.69426454e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.69208753e-01\n",
      "   4.69208753e-01   0.00000000e+00   9.86345729e-01   9.86345729e-01\n",
      "   9.86345729e-01   2.17700959e-04   7.64730071e-01   7.64730071e-01\n",
      "   4.69426454e-01   6.16735242e-01   6.16735242e-01   2.17700959e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.69208753e-01\n",
      "   4.69208753e-01   1.45555448e+00   9.86345729e-01   9.86345729e-01\n",
      "   9.86563430e-01   4.69426454e-01   1.23393882e+00   1.23393882e+00\n",
      "   4.69426454e-01   6.16735242e-01   1.47308788e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.69208753e-01\n",
      "   4.69208753e-01   1.45555448e+00   1.45577218e+00   1.45577218e+00\n",
      "   1.45577218e+00   4.69426454e-01   1.23393882e+00   1.23393882e+00\n",
      "   4.69208753e-01   1.47308788e-01   1.47308788e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.69426454e-01\n",
      "   4.69426454e-01   1.45577218e+00   1.45577218e+00   1.45577218e+00\n",
      "   4.69426454e-01   4.69426454e-01   1.23372112e+00   7.64512370e-01\n",
      "   0.00000000e+00   1.47308788e-01   1.47308788e-01   1.47308788e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.17700959e-04   2.17700959e-04   2.17700959e-04\n",
      "   2.17700959e-04   1.45577218e+00   1.45577218e+00   1.45577218e+00\n",
      "   4.69426454e-01   0.00000000e+00   7.64512370e-01   7.64512370e-01\n",
      "   0.00000000e+00   1.47308788e-01   1.47308788e-01   1.47308788e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.17700959e-04   2.17700959e-04   2.17700959e-04   2.17700959e-04\n",
      "   2.17700959e-04   9.86563430e-01   9.86563430e-01   9.86345729e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.64512370e-01\n",
      "   7.64512370e-01   0.00000000e+00   1.47308788e-01   1.47308788e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.64512370e-01\n",
      "   7.64512370e-01   7.64512370e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.64512370e-01   7.64512370e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  5.04683880e-08   1.41708490e-08   1.38219866e-10   6.72575514e-09\n",
      "   1.87373716e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.41708490e-08   1.41708490e-08   1.41708490e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.41708490e-08\n",
      "   1.41708490e-08   6.46392370e-08   6.46392370e-08   2.08966042e-08\n",
      "   5.71941431e-08   5.06066079e-08   5.04683880e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.04683880e-08\n",
      "   5.04683880e-08   5.04683880e-08   6.46392370e-08   6.46392370e-08\n",
      "   6.46392370e-08   6.46392370e-08   7.13649922e-08   7.13649922e-08\n",
      "   5.71941431e-08   5.06066079e-08   5.04683880e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.38219866e-10   0.00000000e+00   0.00000000e+00\n",
      "   5.04683880e-08   5.04683880e-08   5.04683880e-08   5.04683880e-08\n",
      "   5.04683880e-08   6.46579744e-08   6.46579744e-08   6.46579744e-08\n",
      "   6.46579744e-08   6.46392370e-08   6.72575514e-09   2.09153416e-08\n",
      "   2.10348241e-08   1.38219866e-10   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.38219866e-10   1.38219866e-10   0.00000000e+00   0.00000000e+00\n",
      "   5.04683880e-08   5.04683880e-08   5.04683880e-08   6.46392370e-08\n",
      "   6.46579744e-08   6.46579744e-08   6.46579744e-08   6.46579744e-08\n",
      "   6.46579744e-08   7.13837296e-08   6.74449252e-09   2.09153416e-08\n",
      "   1.43278063e-08   1.43090689e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.38219866e-10   1.38219866e-10   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.04683880e-08   0.00000000e+00   6.46579744e-08\n",
      "   6.46579744e-08   6.46579744e-08   1.41708490e-08   0.00000000e+00\n",
      "   2.08966042e-08   7.13837296e-08   6.74449252e-09   1.56957238e-10\n",
      "   1.43090689e-08   1.41708490e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.38219866e-10   1.38219866e-10   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.41895864e-08   6.46579744e-08\n",
      "   6.46579744e-08   1.41708490e-08   0.00000000e+00   6.72575514e-09\n",
      "   6.72575514e-09   6.74449252e-09   6.74449252e-09   1.56957238e-10\n",
      "   1.43090689e-08   1.41708490e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.38219866e-10   1.38219866e-10   0.00000000e+00   0.00000000e+00\n",
      "   1.87373716e-11   1.41895864e-08   1.41895864e-08   6.46579744e-08\n",
      "   6.46392370e-08   5.04683880e-08   0.00000000e+00   6.72575514e-09\n",
      "   6.74449252e-09   6.74449252e-09   1.87373716e-11   1.38219866e-10\n",
      "   1.43090689e-08   1.41708490e-08   1.41708490e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.38219866e-10\n",
      "   1.38219866e-10   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.41895864e-08   1.41895864e-08   1.41895864e-08   0.00000000e+00\n",
      "   5.04683880e-08   5.04683880e-08   6.72575514e-09   6.72575514e-09\n",
      "   6.74449252e-09   1.87373716e-11   1.56957238e-10   1.38219866e-10\n",
      "   1.43090689e-08   1.41708490e-08   1.41708490e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.38219866e-10\n",
      "   1.38219866e-10   0.00000000e+00   0.00000000e+00   1.87373716e-11\n",
      "   1.41895864e-08   1.41895864e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.04683880e-08   5.72128805e-08   5.72128805e-08\n",
      "   6.74449252e-09   1.56957238e-10   1.38219866e-10   1.38219866e-10\n",
      "   1.41708490e-08   1.41708490e-08   1.41708490e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.38219866e-10\n",
      "   1.38219866e-10   0.00000000e+00   0.00000000e+00   1.41895864e-08\n",
      "   1.41895864e-08   1.41708490e-08   0.00000000e+00   0.00000000e+00\n",
      "   1.56957238e-10   6.88271238e-09   5.73511004e-08   5.73511004e-08\n",
      "   5.06253452e-08   1.38219866e-10   1.38219866e-10   1.38219866e-10\n",
      "   1.41708490e-08   1.41708490e-08   1.41708490e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.38219866e-10\n",
      "   1.38219866e-10   1.38219866e-10   1.38219866e-10   1.43278063e-08\n",
      "   1.43278063e-08   1.56957238e-10   1.56957238e-10   1.56957238e-10\n",
      "   6.88271238e-09   6.88271238e-09   6.88271238e-09   5.72128805e-08\n",
      "   5.04871254e-08   5.04683880e-08   5.06066079e-08   1.38219866e-10\n",
      "   1.41708490e-08   1.41708490e-08   1.41708490e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.38219866e-10   1.38219866e-10   1.43090689e-08\n",
      "   1.43278063e-08   1.56957238e-10   1.87373716e-11   6.74449252e-09\n",
      "   6.72575514e-09   6.72575514e-09   6.74449252e-09   1.87373716e-11\n",
      "   1.87373716e-11   5.04683880e-08   5.06066079e-08   6.47774569e-08\n",
      "   1.41708490e-08   1.41708490e-08   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.41708490e-08\n",
      "   1.41708490e-08   0.00000000e+00   0.00000000e+00   6.72575514e-09\n",
      "   6.72575514e-09   6.72575514e-09   6.74449252e-09   1.87373716e-11\n",
      "   0.00000000e+00   5.06066079e-08   6.47774569e-08   6.47774569e-08\n",
      "   1.41708490e-08   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.41708490e-08\n",
      "   1.41708490e-08   0.00000000e+00   6.72575514e-09   6.72575514e-09\n",
      "   6.72575514e-09   6.72575514e-09   1.87373716e-11   5.04871254e-08\n",
      "   5.04683880e-08   6.47774569e-08   6.47774569e-08   6.46392370e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.41708490e-08\n",
      "   1.41708490e-08   0.00000000e+00   6.72575514e-09   6.72575514e-09\n",
      "   6.72575514e-09   5.04683880e-08   5.04871254e-08   5.04871254e-08\n",
      "   6.46392370e-08   6.47774569e-08   6.47774569e-08   5.04683880e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.41708490e-08\n",
      "   1.41708490e-08   2.08966042e-08   6.72575514e-09   6.72575514e-09\n",
      "   5.71941431e-08   6.46392370e-08   6.46579744e-08   6.46579744e-08\n",
      "   6.46392370e-08   6.47774569e-08   1.38219866e-10   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.41708490e-08\n",
      "   1.41708490e-08   2.08966042e-08   7.13649922e-08   7.13649922e-08\n",
      "   7.13649922e-08   6.46392370e-08   6.46579744e-08   6.46579744e-08\n",
      "   1.41708490e-08   1.38219866e-10   1.38219866e-10   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   6.46392370e-08\n",
      "   6.46392370e-08   7.13649922e-08   7.13649922e-08   7.13649922e-08\n",
      "   6.46392370e-08   6.46392370e-08   1.41895864e-08   1.87373716e-11\n",
      "   0.00000000e+00   1.38219866e-10   1.38219866e-10   1.38219866e-10\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.04683880e-08   5.04683880e-08   5.04683880e-08\n",
      "   5.04683880e-08   7.13649922e-08   7.13649922e-08   7.13649922e-08\n",
      "   6.46392370e-08   0.00000000e+00   1.87373716e-11   1.87373716e-11\n",
      "   0.00000000e+00   1.38219866e-10   1.38219866e-10   1.38219866e-10\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.04683880e-08   5.04683880e-08   5.04683880e-08   5.04683880e-08\n",
      "   5.04683880e-08   5.71941431e-08   5.71941431e-08   6.72575514e-09\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.87373716e-11\n",
      "   1.87373716e-11   0.00000000e+00   1.38219866e-10   1.38219866e-10\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.87373716e-11\n",
      "   1.87373716e-11   1.87373716e-11   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.87373716e-11   1.87373716e-11   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  1.29987665e-07   9.71359514e-02   5.24929072e-03   6.62203653e-06\n",
      "   2.30290402e-06]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.71359514e-02   9.71359514e-02   9.71359514e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.71359514e-02\n",
      "   9.71359514e-02   9.71360814e-02   9.71360814e-02   9.71425735e-02\n",
      "   6.75202420e-06   5.24942071e-03   1.29987665e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.29987665e-07\n",
      "   1.29987665e-07   1.29987665e-07   9.71360814e-02   9.71360814e-02\n",
      "   9.71360814e-02   9.71360814e-02   9.71427034e-02   9.71427034e-02\n",
      "   6.75202420e-06   5.24942071e-03   1.29987665e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.24929072e-03   0.00000000e+00   0.00000000e+00\n",
      "   1.29987665e-07   1.29987665e-07   1.29987665e-07   1.29987665e-07\n",
      "   1.29987665e-07   9.71383843e-02   9.71383843e-02   9.71383843e-02\n",
      "   9.71383843e-02   9.71360814e-02   6.62203653e-06   9.71448764e-02\n",
      "   1.02391864e-01   5.24929072e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.24929072e-03   5.24929072e-03   0.00000000e+00   0.00000000e+00\n",
      "   1.29987665e-07   1.29987665e-07   1.29987665e-07   9.71360814e-02\n",
      "   9.71383843e-02   9.71383843e-02   9.71383843e-02   9.71383843e-02\n",
      "   9.71383843e-02   9.71450063e-02   8.92494056e-06   9.71448764e-02\n",
      "   1.02387545e-01   1.02385242e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.24929072e-03   5.24929072e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.29987665e-07   0.00000000e+00   9.71383843e-02\n",
      "   9.71383843e-02   9.71383843e-02   9.71359514e-02   0.00000000e+00\n",
      "   9.71425735e-02   9.71450063e-02   8.92494056e-06   5.25159363e-03\n",
      "   1.02385242e-01   9.71359514e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.24929072e-03   5.24929072e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   9.71382543e-02   9.71383843e-02\n",
      "   9.71383843e-02   9.71359514e-02   0.00000000e+00   6.62203653e-06\n",
      "   6.62203653e-06   8.92494056e-06   8.92494056e-06   5.25159363e-03\n",
      "   1.02385242e-01   9.71359514e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.24929072e-03   5.24929072e-03   0.00000000e+00   0.00000000e+00\n",
      "   2.30290402e-06   9.71382543e-02   9.71382543e-02   9.71383843e-02\n",
      "   9.71360814e-02   1.29987665e-07   0.00000000e+00   6.62203653e-06\n",
      "   8.92494056e-06   8.92494056e-06   2.30290402e-06   5.24929072e-03\n",
      "   1.02385242e-01   9.71359514e-02   9.71359514e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.24929072e-03\n",
      "   5.24929072e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.71382543e-02   9.71382543e-02   9.71382543e-02   0.00000000e+00\n",
      "   1.29987665e-07   1.29987665e-07   6.62203653e-06   6.62203653e-06\n",
      "   8.92494056e-06   2.30290402e-06   5.25159363e-03   5.24929072e-03\n",
      "   1.02385242e-01   9.71359514e-02   9.71359514e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.24929072e-03\n",
      "   5.24929072e-03   0.00000000e+00   0.00000000e+00   2.30290402e-06\n",
      "   9.71382543e-02   9.71382543e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.29987665e-07   9.05492822e-06   9.05492822e-06\n",
      "   8.92494056e-06   5.25159363e-03   5.24929072e-03   5.24929072e-03\n",
      "   9.71359514e-02   9.71359514e-02   9.71359514e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.24929072e-03\n",
      "   5.24929072e-03   0.00000000e+00   0.00000000e+00   9.71382543e-02\n",
      "   9.71382543e-02   9.71359514e-02   0.00000000e+00   0.00000000e+00\n",
      "   5.25159363e-03   5.25821566e-03   5.25834565e-03   5.25834565e-03\n",
      "   5.25172362e-03   5.24929072e-03   5.24929072e-03   5.24929072e-03\n",
      "   9.71359514e-02   9.71359514e-02   9.71359514e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.24929072e-03\n",
      "   5.24929072e-03   5.24929072e-03   5.24929072e-03   1.02387545e-01\n",
      "   1.02387545e-01   5.25159363e-03   5.25159363e-03   5.25159363e-03\n",
      "   5.25821566e-03   5.25821566e-03   5.25821566e-03   9.05492822e-06\n",
      "   2.43289169e-06   1.29987665e-07   5.24942071e-03   5.24929072e-03\n",
      "   9.71359514e-02   9.71359514e-02   9.71359514e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.24929072e-03   5.24929072e-03   1.02385242e-01\n",
      "   1.02387545e-01   5.25159363e-03   2.30290402e-06   8.92494056e-06\n",
      "   6.62203653e-06   6.62203653e-06   8.92494056e-06   2.30290402e-06\n",
      "   2.30290402e-06   1.29987665e-07   5.24942071e-03   1.02385372e-01\n",
      "   9.71359514e-02   9.71359514e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.71359514e-02\n",
      "   9.71359514e-02   0.00000000e+00   0.00000000e+00   6.62203653e-06\n",
      "   6.62203653e-06   6.62203653e-06   8.92494056e-06   2.30290402e-06\n",
      "   0.00000000e+00   5.24942071e-03   1.02385372e-01   1.02385372e-01\n",
      "   9.71359514e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.71359514e-02\n",
      "   9.71359514e-02   0.00000000e+00   6.62203653e-06   6.62203653e-06\n",
      "   6.62203653e-06   6.62203653e-06   2.30290402e-06   2.43289169e-06\n",
      "   1.29987665e-07   1.02385372e-01   1.02385372e-01   9.71360814e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.71359514e-02\n",
      "   9.71359514e-02   0.00000000e+00   6.62203653e-06   6.62203653e-06\n",
      "   6.62203653e-06   1.29987665e-07   2.43289169e-06   2.43289169e-06\n",
      "   9.71360814e-02   1.02385372e-01   1.02385372e-01   1.29987665e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.71359514e-02\n",
      "   9.71359514e-02   9.71425735e-02   6.62203653e-06   6.62203653e-06\n",
      "   6.75202420e-06   9.71360814e-02   9.71383843e-02   9.71383843e-02\n",
      "   9.71360814e-02   1.02385372e-01   5.24929072e-03   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.71359514e-02\n",
      "   9.71359514e-02   9.71425735e-02   9.71427034e-02   9.71427034e-02\n",
      "   9.71427034e-02   9.71360814e-02   9.71383843e-02   9.71383843e-02\n",
      "   9.71359514e-02   5.24929072e-03   5.24929072e-03   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.71360814e-02\n",
      "   9.71360814e-02   9.71427034e-02   9.71427034e-02   9.71427034e-02\n",
      "   9.71360814e-02   9.71360814e-02   9.71382543e-02   2.30290402e-06\n",
      "   0.00000000e+00   5.24929072e-03   5.24929072e-03   5.24929072e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.29987665e-07   1.29987665e-07   1.29987665e-07\n",
      "   1.29987665e-07   9.71427034e-02   9.71427034e-02   9.71427034e-02\n",
      "   9.71360814e-02   0.00000000e+00   2.30290402e-06   2.30290402e-06\n",
      "   0.00000000e+00   5.24929072e-03   5.24929072e-03   5.24929072e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.29987665e-07   1.29987665e-07   1.29987665e-07   1.29987665e-07\n",
      "   1.29987665e-07   6.75202420e-06   6.75202420e-06   6.62203653e-06\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.30290402e-06\n",
      "   2.30290402e-06   0.00000000e+00   5.24929072e-03   5.24929072e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.30290402e-06\n",
      "   2.30290402e-06   2.30290402e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.30290402e-06   2.30290402e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  1.96316373e-07   8.17432628e-02   8.47320273e-01   9.07219398e-06\n",
      "   9.20721787e-04]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.17432628e-02   8.17432628e-02   8.17432628e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.17432628e-02\n",
      "   8.17432628e-02   8.17434591e-02   8.17434591e-02   8.17523350e-02\n",
      "   9.26851035e-06   8.47320469e-01   1.96316373e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.96316373e-07\n",
      "   1.96316373e-07   1.96316373e-07   8.17434591e-02   8.17434591e-02\n",
      "   8.17434591e-02   8.17434591e-02   8.17525313e-02   8.17525313e-02\n",
      "   9.26851035e-06   8.47320469e-01   1.96316373e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.47320273e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.96316373e-07   1.96316373e-07   1.96316373e-07   1.96316373e-07\n",
      "   1.96316373e-07   8.26641809e-02   8.26641809e-02   8.26641809e-02\n",
      "   8.26641809e-02   8.17434591e-02   9.07219398e-06   8.26730568e-02\n",
      "   9.29072608e-01   8.47320273e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.47320273e-01   8.47320273e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.96316373e-07   1.96316373e-07   1.96316373e-07   8.17434591e-02\n",
      "   8.26641809e-02   8.26641809e-02   8.26641809e-02   8.26641809e-02\n",
      "   8.26641809e-02   8.26732531e-02   9.29793981e-04   8.26730568e-02\n",
      "   9.29984257e-01   9.29063536e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.47320273e-01   8.47320273e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.96316373e-07   0.00000000e+00   8.26641809e-02\n",
      "   8.26641809e-02   8.26641809e-02   8.17432628e-02   0.00000000e+00\n",
      "   8.17523350e-02   8.26732531e-02   9.29793981e-04   8.48240995e-01\n",
      "   9.29063536e-01   8.17432628e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.47320273e-01   8.47320273e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   8.26639846e-02   8.26641809e-02\n",
      "   8.26641809e-02   8.17432628e-02   0.00000000e+00   9.07219398e-06\n",
      "   9.07219398e-06   9.29793981e-04   9.29793981e-04   8.48240995e-01\n",
      "   9.29063536e-01   8.17432628e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.47320273e-01   8.47320273e-01   0.00000000e+00   0.00000000e+00\n",
      "   9.20721787e-04   8.26639846e-02   8.26639846e-02   8.26641809e-02\n",
      "   8.17434591e-02   1.96316373e-07   0.00000000e+00   9.07219398e-06\n",
      "   9.29793981e-04   9.29793981e-04   9.20721787e-04   8.47320273e-01\n",
      "   9.29063536e-01   8.17432628e-02   8.17432628e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.47320273e-01\n",
      "   8.47320273e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.26639846e-02   8.26639846e-02   8.26639846e-02   0.00000000e+00\n",
      "   1.96316373e-07   1.96316373e-07   9.07219398e-06   9.07219398e-06\n",
      "   9.29793981e-04   9.20721787e-04   8.48240995e-01   8.47320273e-01\n",
      "   9.29063536e-01   8.17432628e-02   8.17432628e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.47320273e-01\n",
      "   8.47320273e-01   0.00000000e+00   0.00000000e+00   9.20721787e-04\n",
      "   8.26639846e-02   8.26639846e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.96316373e-07   9.29990298e-04   9.29990298e-04\n",
      "   9.29793981e-04   8.48240995e-01   8.47320273e-01   8.47320273e-01\n",
      "   8.17432628e-02   8.17432628e-02   8.17432628e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.47320273e-01\n",
      "   8.47320273e-01   0.00000000e+00   0.00000000e+00   8.26639846e-02\n",
      "   8.26639846e-02   8.17432628e-02   0.00000000e+00   0.00000000e+00\n",
      "   8.48240995e-01   8.48250067e-01   8.48250263e-01   8.48250263e-01\n",
      "   8.48241191e-01   8.47320273e-01   8.47320273e-01   8.47320273e-01\n",
      "   8.17432628e-02   8.17432628e-02   8.17432628e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.47320273e-01\n",
      "   8.47320273e-01   8.47320273e-01   8.47320273e-01   9.29984257e-01\n",
      "   9.29984257e-01   8.48240995e-01   8.48240995e-01   8.48240995e-01\n",
      "   8.48250067e-01   8.48250067e-01   8.48250067e-01   9.29990298e-04\n",
      "   9.20918104e-04   1.96316373e-07   8.47320469e-01   8.47320273e-01\n",
      "   8.17432628e-02   8.17432628e-02   8.17432628e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.47320273e-01   8.47320273e-01   9.29063536e-01\n",
      "   9.29984257e-01   8.48240995e-01   9.20721787e-04   9.29793981e-04\n",
      "   9.07219398e-06   9.07219398e-06   9.29793981e-04   9.20721787e-04\n",
      "   9.20721787e-04   1.96316373e-07   8.47320469e-01   9.29063732e-01\n",
      "   8.17432628e-02   8.17432628e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.17432628e-02\n",
      "   8.17432628e-02   0.00000000e+00   0.00000000e+00   9.07219398e-06\n",
      "   9.07219398e-06   9.07219398e-06   9.29793981e-04   9.20721787e-04\n",
      "   0.00000000e+00   8.47320469e-01   9.29063732e-01   9.29063732e-01\n",
      "   8.17432628e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.17432628e-02\n",
      "   8.17432628e-02   0.00000000e+00   9.07219398e-06   9.07219398e-06\n",
      "   9.07219398e-06   9.07219398e-06   9.20721787e-04   9.20918104e-04\n",
      "   1.96316373e-07   9.29063732e-01   9.29063732e-01   8.17434591e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.17432628e-02\n",
      "   8.17432628e-02   0.00000000e+00   9.07219398e-06   9.07219398e-06\n",
      "   9.07219398e-06   1.96316373e-07   9.20918104e-04   9.20918104e-04\n",
      "   8.17434591e-02   9.29063732e-01   9.29063732e-01   1.96316373e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.17432628e-02\n",
      "   8.17432628e-02   8.17523350e-02   9.07219398e-06   9.07219398e-06\n",
      "   9.26851035e-06   8.17434591e-02   8.26641809e-02   8.26641809e-02\n",
      "   8.17434591e-02   9.29063732e-01   8.47320273e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.17432628e-02\n",
      "   8.17432628e-02   8.17523350e-02   8.17525313e-02   8.17525313e-02\n",
      "   8.17525313e-02   8.17434591e-02   8.26641809e-02   8.26641809e-02\n",
      "   8.17432628e-02   8.47320273e-01   8.47320273e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.17434591e-02\n",
      "   8.17434591e-02   8.17525313e-02   8.17525313e-02   8.17525313e-02\n",
      "   8.17434591e-02   8.17434591e-02   8.26639846e-02   9.20721787e-04\n",
      "   0.00000000e+00   8.47320273e-01   8.47320273e-01   8.47320273e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.96316373e-07   1.96316373e-07   1.96316373e-07\n",
      "   1.96316373e-07   8.17525313e-02   8.17525313e-02   8.17525313e-02\n",
      "   8.17434591e-02   0.00000000e+00   9.20721787e-04   9.20721787e-04\n",
      "   0.00000000e+00   8.47320273e-01   8.47320273e-01   8.47320273e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.96316373e-07   1.96316373e-07   1.96316373e-07   1.96316373e-07\n",
      "   1.96316373e-07   9.26851035e-06   9.26851035e-06   9.07219398e-06\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.20721787e-04\n",
      "   9.20721787e-04   0.00000000e+00   8.47320273e-01   8.47320273e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.20721787e-04\n",
      "   9.20721787e-04   9.20721787e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.20721787e-04   9.20721787e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  3.20131283e-07   7.59441996e-05   4.13951342e-05   1.17997795e-02\n",
      "   1.48381433e-06]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.59441996e-05   7.59441996e-05   7.59441996e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.59441996e-05\n",
      "   7.59441996e-05   7.62643309e-05   7.62643309e-05   1.18757237e-02\n",
      "   1.18000997e-02   4.17152655e-05   3.20131283e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.20131283e-07\n",
      "   3.20131283e-07   3.20131283e-07   7.62643309e-05   7.62643309e-05\n",
      "   7.62643309e-05   7.62643309e-05   1.18760439e-02   1.18760439e-02\n",
      "   1.18000997e-02   4.17152655e-05   3.20131283e-07   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   4.13951342e-05   0.00000000e+00   0.00000000e+00\n",
      "   3.20131283e-07   3.20131283e-07   3.20131283e-07   3.20131283e-07\n",
      "   3.20131283e-07   7.77481452e-05   7.77481452e-05   7.77481452e-05\n",
      "   7.77481452e-05   7.62643309e-05   1.17997795e-02   1.18772075e-02\n",
      "   1.19171189e-02   4.13951342e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.13951342e-05   4.13951342e-05   0.00000000e+00   0.00000000e+00\n",
      "   3.20131283e-07   3.20131283e-07   3.20131283e-07   7.62643309e-05\n",
      "   7.77481452e-05   7.77481452e-05   7.77481452e-05   7.77481452e-05\n",
      "   7.77481452e-05   1.18775277e-02   1.18012633e-02   1.18772075e-02\n",
      "   1.18823148e-04   1.17339334e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.13951342e-05   4.13951342e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.20131283e-07   0.00000000e+00   7.77481452e-05\n",
      "   7.77481452e-05   7.77481452e-05   7.59441996e-05   0.00000000e+00\n",
      "   1.18757237e-02   1.18775277e-02   1.18012633e-02   4.28789486e-05\n",
      "   1.17339334e-04   7.59441996e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.13951342e-05   4.13951342e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.74280140e-05   7.77481452e-05\n",
      "   7.77481452e-05   7.59441996e-05   0.00000000e+00   1.17997795e-02\n",
      "   1.17997795e-02   1.18012633e-02   1.18012633e-02   4.28789486e-05\n",
      "   1.17339334e-04   7.59441996e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.13951342e-05   4.13951342e-05   0.00000000e+00   0.00000000e+00\n",
      "   1.48381433e-06   7.74280140e-05   7.74280140e-05   7.77481452e-05\n",
      "   7.62643309e-05   3.20131283e-07   0.00000000e+00   1.17997795e-02\n",
      "   1.18012633e-02   1.18012633e-02   1.48381433e-06   4.13951342e-05\n",
      "   1.17339334e-04   7.59441996e-05   7.59441996e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.13951342e-05\n",
      "   4.13951342e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.74280140e-05   7.74280140e-05   7.74280140e-05   0.00000000e+00\n",
      "   3.20131283e-07   3.20131283e-07   1.17997795e-02   1.17997795e-02\n",
      "   1.18012633e-02   1.48381433e-06   4.28789486e-05   4.13951342e-05\n",
      "   1.17339334e-04   7.59441996e-05   7.59441996e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.13951342e-05\n",
      "   4.13951342e-05   0.00000000e+00   0.00000000e+00   1.48381433e-06\n",
      "   7.74280140e-05   7.74280140e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.20131283e-07   1.18015835e-02   1.18015835e-02\n",
      "   1.18012633e-02   4.28789486e-05   4.13951342e-05   4.13951342e-05\n",
      "   7.59441996e-05   7.59441996e-05   7.59441996e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.13951342e-05\n",
      "   4.13951342e-05   0.00000000e+00   0.00000000e+00   7.74280140e-05\n",
      "   7.74280140e-05   7.59441996e-05   0.00000000e+00   0.00000000e+00\n",
      "   4.28789486e-05   1.18426585e-02   1.18429786e-02   1.18429786e-02\n",
      "   4.31990799e-05   4.13951342e-05   4.13951342e-05   4.13951342e-05\n",
      "   7.59441996e-05   7.59441996e-05   7.59441996e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.13951342e-05\n",
      "   4.13951342e-05   4.13951342e-05   4.13951342e-05   1.18823148e-04\n",
      "   1.18823148e-04   4.28789486e-05   4.28789486e-05   4.28789486e-05\n",
      "   1.18426585e-02   1.18426585e-02   1.18426585e-02   1.18015835e-02\n",
      "   1.80394561e-06   3.20131283e-07   4.17152655e-05   4.13951342e-05\n",
      "   7.59441996e-05   7.59441996e-05   7.59441996e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   4.13951342e-05   4.13951342e-05   1.17339334e-04\n",
      "   1.18823148e-04   4.28789486e-05   1.48381433e-06   1.18012633e-02\n",
      "   1.17997795e-02   1.17997795e-02   1.18012633e-02   1.48381433e-06\n",
      "   1.48381433e-06   3.20131283e-07   4.17152655e-05   1.17659465e-04\n",
      "   7.59441996e-05   7.59441996e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.59441996e-05\n",
      "   7.59441996e-05   0.00000000e+00   0.00000000e+00   1.17997795e-02\n",
      "   1.17997795e-02   1.17997795e-02   1.18012633e-02   1.48381433e-06\n",
      "   0.00000000e+00   4.17152655e-05   1.17659465e-04   1.17659465e-04\n",
      "   7.59441996e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.59441996e-05\n",
      "   7.59441996e-05   0.00000000e+00   1.17997795e-02   1.17997795e-02\n",
      "   1.17997795e-02   1.17997795e-02   1.48381433e-06   1.80394561e-06\n",
      "   3.20131283e-07   1.17659465e-04   1.17659465e-04   7.62643309e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.59441996e-05\n",
      "   7.59441996e-05   0.00000000e+00   1.17997795e-02   1.17997795e-02\n",
      "   1.17997795e-02   3.20131283e-07   1.80394561e-06   1.80394561e-06\n",
      "   7.62643309e-05   1.17659465e-04   1.17659465e-04   3.20131283e-07\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.59441996e-05\n",
      "   7.59441996e-05   1.18757237e-02   1.17997795e-02   1.17997795e-02\n",
      "   1.18000997e-02   7.62643309e-05   7.77481452e-05   7.77481452e-05\n",
      "   7.62643309e-05   1.17659465e-04   4.13951342e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.59441996e-05\n",
      "   7.59441996e-05   1.18757237e-02   1.18760439e-02   1.18760439e-02\n",
      "   1.18760439e-02   7.62643309e-05   7.77481452e-05   7.77481452e-05\n",
      "   7.59441996e-05   4.13951342e-05   4.13951342e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.62643309e-05\n",
      "   7.62643309e-05   1.18760439e-02   1.18760439e-02   1.18760439e-02\n",
      "   7.62643309e-05   7.62643309e-05   7.74280140e-05   1.48381433e-06\n",
      "   0.00000000e+00   4.13951342e-05   4.13951342e-05   4.13951342e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.20131283e-07   3.20131283e-07   3.20131283e-07\n",
      "   3.20131283e-07   1.18760439e-02   1.18760439e-02   1.18760439e-02\n",
      "   7.62643309e-05   0.00000000e+00   1.48381433e-06   1.48381433e-06\n",
      "   0.00000000e+00   4.13951342e-05   4.13951342e-05   4.13951342e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.20131283e-07   3.20131283e-07   3.20131283e-07   3.20131283e-07\n",
      "   3.20131283e-07   1.18000997e-02   1.18000997e-02   1.17997795e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.48381433e-06\n",
      "   1.48381433e-06   0.00000000e+00   4.13951342e-05   4.13951342e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.48381433e-06\n",
      "   1.48381433e-06   1.48381433e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.48381433e-06   1.48381433e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[  9.99781598e-01   3.51618271e-01   7.17861580e-05   1.82659046e-03\n",
      "   2.34556399e-01]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.51618271e-01   3.51618271e-01   3.51618271e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.51618271e-01\n",
      "   3.51618271e-01   1.35139987e+00   1.35139987e+00   3.53444861e-01\n",
      "   1.00160819e+00   9.99853384e-01   9.99781598e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.99781598e-01\n",
      "   9.99781598e-01   9.99781598e-01   1.35139987e+00   1.35139987e+00\n",
      "   1.35139987e+00   1.35139987e+00   1.35322646e+00   1.35322646e+00\n",
      "   1.00160819e+00   9.99853384e-01   9.99781598e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   7.17861580e-05   0.00000000e+00   0.00000000e+00\n",
      "   9.99781598e-01   9.99781598e-01   9.99781598e-01   9.99781598e-01\n",
      "   9.99781598e-01   1.58595627e+00   1.58595627e+00   1.58595627e+00\n",
      "   1.58595627e+00   1.35139987e+00   1.82659046e-03   5.88001260e-01\n",
      "   3.53516647e-01   7.17861580e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.17861580e-05   7.17861580e-05   0.00000000e+00   0.00000000e+00\n",
      "   9.99781598e-01   9.99781598e-01   9.99781598e-01   1.35139987e+00\n",
      "   1.58595627e+00   1.58595627e+00   1.58595627e+00   1.58595627e+00\n",
      "   1.58595627e+00   1.58778286e+00   2.36382990e-01   5.88001260e-01\n",
      "   5.86246456e-01   3.51690057e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.17861580e-05   7.17861580e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   9.99781598e-01   0.00000000e+00   1.58595627e+00\n",
      "   1.58595627e+00   1.58595627e+00   3.51618271e-01   0.00000000e+00\n",
      "   3.53444861e-01   1.58778286e+00   2.36382990e-01   2.34628185e-01\n",
      "   3.51690057e-01   3.51618271e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.17861580e-05   7.17861580e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.86174670e-01   1.58595627e+00\n",
      "   1.58595627e+00   3.51618271e-01   0.00000000e+00   1.82659046e-03\n",
      "   1.82659046e-03   2.36382990e-01   2.36382990e-01   2.34628185e-01\n",
      "   3.51690057e-01   3.51618271e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.17861580e-05   7.17861580e-05   0.00000000e+00   0.00000000e+00\n",
      "   2.34556399e-01   5.86174670e-01   5.86174670e-01   1.58595627e+00\n",
      "   1.35139987e+00   9.99781598e-01   0.00000000e+00   1.82659046e-03\n",
      "   2.36382990e-01   2.36382990e-01   2.34556399e-01   7.17861580e-05\n",
      "   3.51690057e-01   3.51618271e-01   3.51618271e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.17861580e-05\n",
      "   7.17861580e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.86174670e-01   5.86174670e-01   5.86174670e-01   0.00000000e+00\n",
      "   9.99781598e-01   9.99781598e-01   1.82659046e-03   1.82659046e-03\n",
      "   2.36382990e-01   2.34556399e-01   2.34628185e-01   7.17861580e-05\n",
      "   3.51690057e-01   3.51618271e-01   3.51618271e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.17861580e-05\n",
      "   7.17861580e-05   0.00000000e+00   0.00000000e+00   2.34556399e-01\n",
      "   5.86174670e-01   5.86174670e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   9.99781598e-01   1.23616459e+00   1.23616459e+00\n",
      "   2.36382990e-01   2.34628185e-01   7.17861580e-05   7.17861580e-05\n",
      "   3.51618271e-01   3.51618271e-01   3.51618271e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.17861580e-05\n",
      "   7.17861580e-05   0.00000000e+00   0.00000000e+00   5.86174670e-01\n",
      "   5.86174670e-01   3.51618271e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.34628185e-01   2.36454776e-01   1.23623637e+00   1.23623637e+00\n",
      "   1.23440978e+00   7.17861580e-05   7.17861580e-05   7.17861580e-05\n",
      "   3.51618271e-01   3.51618271e-01   3.51618271e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.17861580e-05\n",
      "   7.17861580e-05   7.17861580e-05   7.17861580e-05   5.86246456e-01\n",
      "   5.86246456e-01   2.34628185e-01   2.34628185e-01   2.34628185e-01\n",
      "   2.36454776e-01   2.36454776e-01   2.36454776e-01   1.23616459e+00\n",
      "   1.23433800e+00   9.99781598e-01   9.99853384e-01   7.17861580e-05\n",
      "   3.51618271e-01   3.51618271e-01   3.51618271e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   7.17861580e-05   7.17861580e-05   3.51690057e-01\n",
      "   5.86246456e-01   2.34628185e-01   2.34556399e-01   2.36382990e-01\n",
      "   1.82659046e-03   1.82659046e-03   2.36382990e-01   2.34556399e-01\n",
      "   2.34556399e-01   9.99781598e-01   9.99853384e-01   1.35147165e+00\n",
      "   3.51618271e-01   3.51618271e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.51618271e-01\n",
      "   3.51618271e-01   0.00000000e+00   0.00000000e+00   1.82659046e-03\n",
      "   1.82659046e-03   1.82659046e-03   2.36382990e-01   2.34556399e-01\n",
      "   0.00000000e+00   9.99853384e-01   1.35147165e+00   1.35147165e+00\n",
      "   3.51618271e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.51618271e-01\n",
      "   3.51618271e-01   0.00000000e+00   1.82659046e-03   1.82659046e-03\n",
      "   1.82659046e-03   1.82659046e-03   2.34556399e-01   1.23433800e+00\n",
      "   9.99781598e-01   1.35147165e+00   1.35147165e+00   1.35139987e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.51618271e-01\n",
      "   3.51618271e-01   0.00000000e+00   1.82659046e-03   1.82659046e-03\n",
      "   1.82659046e-03   9.99781598e-01   1.23433800e+00   1.23433800e+00\n",
      "   1.35139987e+00   1.35147165e+00   1.35147165e+00   9.99781598e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.51618271e-01\n",
      "   3.51618271e-01   3.53444861e-01   1.82659046e-03   1.82659046e-03\n",
      "   1.00160819e+00   1.35139987e+00   1.58595627e+00   1.58595627e+00\n",
      "   1.35139987e+00   1.35147165e+00   7.17861580e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.51618271e-01\n",
      "   3.51618271e-01   3.53444861e-01   1.35322646e+00   1.35322646e+00\n",
      "   1.35322646e+00   1.35139987e+00   1.58595627e+00   1.58595627e+00\n",
      "   3.51618271e-01   7.17861580e-05   7.17861580e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.35139987e+00\n",
      "   1.35139987e+00   1.35322646e+00   1.35322646e+00   1.35322646e+00\n",
      "   1.35139987e+00   1.35139987e+00   5.86174670e-01   2.34556399e-01\n",
      "   0.00000000e+00   7.17861580e-05   7.17861580e-05   7.17861580e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   9.99781598e-01   9.99781598e-01   9.99781598e-01\n",
      "   9.99781598e-01   1.35322646e+00   1.35322646e+00   1.35322646e+00\n",
      "   1.35139987e+00   0.00000000e+00   2.34556399e-01   2.34556399e-01\n",
      "   0.00000000e+00   7.17861580e-05   7.17861580e-05   7.17861580e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.99781598e-01   9.99781598e-01   9.99781598e-01   9.99781598e-01\n",
      "   9.99781598e-01   1.00160819e+00   1.00160819e+00   1.82659046e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.34556399e-01\n",
      "   2.34556399e-01   0.00000000e+00   7.17861580e-05   7.17861580e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.34556399e-01\n",
      "   2.34556399e-01   2.34556399e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.34556399e-01   2.34556399e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "5.0 5\n",
      "[debug] enumerator first part: [[-0.69314718 -0.91629073]\n",
      " [ 0.          0.        ]\n",
      " [-2.30258509 -0.51082562]\n",
      " [-4.60517019 -3.72970145]]\n",
      "[debug] enumerator second part: [[-0.32850407 -1.02165125]\n",
      " [-1.02165125 -1.53247687]\n",
      " [-0.91629073 -0.61618614]\n",
      " [ 0.          0.        ]]\n",
      "[debug] enumerator first+second part: [[-1.02165125 -1.93794198]\n",
      " [-1.02165125 -1.53247687]\n",
      " [-3.21887582 -1.12701176]\n",
      " [-4.60517019 -3.72970145]]\n",
      "[debug] enumerator third part: [-0.69314718 -0.69314718]\n",
      "\n",
      "[debug] enumerator [[-1.71479843 -2.63108916]\n",
      " [-1.71479843 -2.22562405]\n",
      " [-3.91202301 -1.82015894]\n",
      " [-5.29831737 -4.42284863]]\n",
      "[debug] denominator [[-1.37832619]\n",
      " [-1.2447948 ]\n",
      " [-1.70374859]\n",
      " [-4.07454193]]\n",
      "[debug] enumerator-denominator [[-0.33647224 -1.25276297]\n",
      " [-0.47000363 -0.98082925]\n",
      " [-2.20827441 -0.11641035]\n",
      " [-1.22377543 -0.34830669]]\n",
      "\n",
      "[ 0.71428571  0.625       0.10989011  0.29411765]\n",
      "[ 0.29411765  0.40400776  1.00840336]\n",
      "[ 0.28571429  0.375       0.89010989  0.70588235]\n",
      "[ 0.70588235  1.59599224  0.99159664]\n",
      "4.0 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.16871379,  0.23174971,  0.57844728],\n",
       "        [ 0.31279315,  0.70722188,  0.43939991]]),\n",
       " array([ 0.43582337,  0.56417663]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh, let's test again\n",
    "mu_test, pi_test = M_step(X_test, gamma_test)\n",
    "\n",
    "assert mu_test.shape == (K_test,D_test)\n",
    "assert pi_test.shape == (K_test, )\n",
    "\n",
    "M_step(X_naive, E_step(X_naive, MU_naive, pi_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-46.051701859880914"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f60d48b8b22063cef560b42944a0aa4",
     "grade": true,
     "grade_id": "cell-6e7c751b30acfd45",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Oh, let's test again\n",
    "mu_test, pi_test = M_step(X_test, gamma_test)\n",
    "\n",
    "assert mu_test.shape == (K_test,D_test)\n",
    "assert pi_test.shape == (K_test, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_x_mu(X, MU):\n",
    "    '''Log and matrix version of bishop 9.48'''\n",
    "    \n",
    "    # X = [N, D]\n",
    "    # MU = [K, D]\n",
    "    # return [N, K]\n",
    "    \n",
    "    stabilizer = 1e-3\n",
    "    return X @ np.log(MU.T + stabilizer) + (1.0-X) @ np.log(1 - MU.T + stabilizer)\n",
    "\n",
    "\n",
    "def log_likelihood(gamma, pi, X, mu):\n",
    "    \n",
    "    # [1, K]\n",
    "    log_pi = np.log(pi + 1e-12).reshape(1, -1)\n",
    "    \n",
    "#     print(\"log pi\", log_pi)\n",
    "    \n",
    "    # [N, K]\n",
    "    weighted_log_probs = log_pi + log_x_mu(X, mu)\n",
    "    \n",
    "#     print(\"log x mu\", log_x_mu(X, mu))\n",
    "    \n",
    "    # Element wise multiplication between gamma and weighted\n",
    "    # log-probabilities and then summed over N and K\n",
    "    result = np.sum(gamma * weighted_log_probs)\n",
    "    \n",
    "#     print(\"gamma * weighted\", gamma * weighted_log_probs)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2845.3372376427637"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood(gamma_test, pi_test, X_test, mu_test)\n",
    "# E_step(X_test, mu_test, pi_test)\n",
    "# M_step(X_test, gamma_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "acfec6384b058cb0ce1932006fbfebc4",
     "grade": true,
     "grade_id": "cell-d6c4368246dee7e6",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def EM(X, K, max_iter, mu=None, pi=None):\n",
    "    \n",
    "    # 'Calculate' N and D\n",
    "    N,D = X.shape\n",
    "    \n",
    "    # Init\n",
    "    pi = np.ones(K)/K\n",
    "    best_log_likelihood = -inf\n",
    "    epsilon = 1e-6 # change in likelihood must be bigger than epsilon\n",
    "    mu = np.random.uniform(low=0.25, high=0.75, size=(K,D))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        print('itaration: ', i,'\\n\\n')\n",
    "        #perform E-step\n",
    "        gamma = E_step(X,mu,pi)\n",
    "        \n",
    "        print(\"Summed gamma\", np.sum(gamma, axis=1))\n",
    "        \n",
    "#         print(\"E-step gamma\", gamma)\n",
    "        \n",
    "        #perform M-step\n",
    "        mu,pi = M_step(X, gamma)\n",
    "        \n",
    "        print(\"Summed pi\", np.sum(pi))\n",
    "                \n",
    "#         print(\"M-step mu and pi\", (mu, pi))\n",
    "        \n",
    "        log_like = log_likelihood(gamma, pi, X, mu)\n",
    "        delta = log_like - best_log_likelihood\n",
    "        \n",
    "#         print('log-like: ', log_like)\n",
    "#         print('best log like: ',best_log_likelihood)\n",
    "        # Keep track of log likelihood\n",
    "    \n",
    "        if delta > 0:\n",
    "            best_log_likelihood = log_like\n",
    "            \n",
    "            # Stop on convergence\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "            \n",
    "    \n",
    "    return mu, gamma, best_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itaration:  0 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  1 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  2 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  3 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  4 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  5 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  6 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  7 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  8 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  9 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  10 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  11 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  12 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  13 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  14 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  15 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  16 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  17 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  18 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  19 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  20 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  21 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  22 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  23 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  24 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  25 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  26 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  27 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  28 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  29 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  30 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  31 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  32 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  33 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  34 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  35 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  36 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  37 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  38 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  39 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  40 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  41 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  42 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  43 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  44 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  45 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  46 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  47 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  48 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  49 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  50 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  51 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  52 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  53 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  54 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  55 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  56 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  57 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  58 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  59 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  60 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  61 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  62 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  63 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  64 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  65 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed pi 1.0\n",
      "itaration:  66 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  67 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  68 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  69 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  70 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  71 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  72 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  73 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  74 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  75 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  76 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  77 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  78 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  79 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  80 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  81 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  82 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  83 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  84 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  85 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  86 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  87 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  88 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  89 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  90 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  91 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  92 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  93 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  94 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  95 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  96 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  97 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  98 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n",
      "itaration:  99 \n",
      "\n",
      "\n",
      "Summed gamma [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Summed pi 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x634aa7a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10\n",
    "D = 2\n",
    "K = 3\n",
    "\n",
    "c1 = np.random.normal(loc=random.uniform(size=(1,D)), scale=np.diag(np.random.uniform(size=(1,D))), size=(N,D))\n",
    "c2 = np.random.normal(loc=random.uniform(size=(1,D)), scale=np.diag(np.random.uniform(size=(1,D))), size=(N,D))\n",
    "c3 = np.random.normal(loc=random.uniform(size=(1,D)), scale=np.diag(np.random.uniform(size=(1,D))), size=(N,D))\n",
    "\n",
    "mock_data = np.concatenate((c1, c2, c2), axis=0)\n",
    "\n",
    "figure\n",
    "plt.scatter(mock_data[:, 0], mock_data[:, 1])\n",
    "\n",
    "mu, gamma, _ = EM(mock_data, K, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31851952  0.74182174]\n",
      " [ 0.31851952  0.74182174]\n",
      " [ 0.31851952  0.74182174]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1335dbf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure\n",
    "plt.scatter(mock_data[:, 0], mock_data[:, 1])\n",
    "plt.scatter(mu[:, 0], mu[:, 1], marker='o')\n",
    "\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4fc12faa0da660f7a4d9cc7deb41b25",
     "grade": false,
     "grade_id": "cell-e1077ed3b83489be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 Three digits experiment (10 points)\n",
    "In analogue with Bishop $\\S9.3.3$, sample a training set consisting of only __binary__ images of written digits $2$, $3$, and $4$. Run your EM algorithm and show the reconstructed digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bdbce0fad0ed151063d4c489ce999e3e",
     "grade": true,
     "grade_id": "cell-477155d0264d7259",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "np.exp(-21)\n",
    "np.log(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "485543f4893938d2a9dc1c17d8221cbc",
     "grade": false,
     "grade_id": "cell-88c9664f995b1909",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Can you identify which element in the latent space corresponds to which digit? What are the identified mixing coefficients for digits $2$, $3$ and $4$, and how do these compare to the true ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ae7b5acea6089e2590059f90b0d0a0be",
     "grade": true,
     "grade_id": "cell-3680ae2159c48193",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98e04feb59a36867367b3027df9e226d",
     "grade": false,
     "grade_id": "cell-0891dda1c3e80e9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Experiments (20 points)\n",
    "Perform the follow-up experiments listed below using your implementation of the EM algorithm. For each of these, describe/comment on the obtained results and give an explanation. You may still use your dataset with only digits 2, 3 and 4 as otherwise computations can take very long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "439067186fa3ef1d7261a9bcf5a84ea6",
     "grade": false,
     "grade_id": "cell-06fe1b1355689928",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.1 Size of the latent space (5 points)\n",
    "Run EM with $K$ larger or smaller than the true number of classes. Describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "791512aeadd30c4b586b966ca10e6fad",
     "grade": true,
     "grade_id": "cell-6c9057f2546b7215",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e12e40c2d2165e3bb500b5504128910d",
     "grade": true,
     "grade_id": "cell-f01c37653160244b",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b306681523a2e35eea310ac10bb68999",
     "grade": false,
     "grade_id": "cell-cf478d67239b7f2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.2 Identify misclassifications (10 points)\n",
    "How can you use the data labels to assign a label to each of the clusters/latent variables? Use this to identify images that are 'misclassified' and try to understand why they are. Report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "000c11bd8756a4e24296c7c55d3ee17e",
     "grade": true,
     "grade_id": "cell-daa1a492fbba5c7e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "baf43434481c13d76ad51e3ba07e2bf5",
     "grade": true,
     "grade_id": "cell-329245c02df7850d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "640bc57a2d08c3becf534bb5e4b35971",
     "grade": false,
     "grade_id": "cell-67ce1222e8a7837b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.3 Initialize with true values (5 points)\n",
    "Initialize the three classes with the true values of the parameters and see what happens. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a48f788e286458ef0f776865a3bcd58b",
     "grade": true,
     "grade_id": "cell-aa5d6b9f941d985d",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1dc4adf3081f3bec93f94c3b12b87db9",
     "grade": true,
     "grade_id": "cell-981e44f35a3764b0",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd613f41e5d2b7d22b0d5b1e7644a48a",
     "grade": false,
     "grade_id": "cell-19bfd7cf4017ed84",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Variational Auto-Encoder\n",
    "\n",
    "A Variational Auto-Encoder (VAE) is a probabilistic model $p(\\bx, \\bz)$ over observed variables $\\bx$ and latent variables and/or parameters $\\bz$. Here we distinguish the decoder part, $p(\\bx | \\bz) p(\\bz)$ and an encoder part $p(\\bz | \\bx)$ that are both specified with a neural network. A lower bound on the log marginal likelihood $\\log p(\\bx)$ can be obtained by approximately inferring the latent variables z from the observed data x using an encoder distribution $q(\\bz| \\bx)$ that is also specified as a neural network. This lower bound is then optimized to fit the model to the data. \n",
    "\n",
    "The model was introduced by Diederik Kingma (during his PhD at the UVA) and Max Welling in 2013, https://arxiv.org/abs/1312.6114. \n",
    "\n",
    "Since it is such an important model there are plenty of well written tutorials that should help you with the assignment. E.g: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/.\n",
    "\n",
    "In the following, we will make heavily use of the torch module, https://pytorch.org/docs/stable/index.html. Most of the time replacing `np.` with `torch.` will do the trick, e.g. `np.sum` becomes `torch.sum` and `np.log` becomes `torch.log`. In addition, we will use `torch.FloatTensor()` as an equivalent to `np.array()`. In order to train our VAE efficiently we will make use of batching. The number of data points in a batch will become the first dimension of our data tensor, e.g. A batch of 128 MNIST images has the dimensions [128, 1, 28, 28]. To check check the dimensions of a tensor you can call `.size()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92bd337f41c3f94777f47376c7149ca7",
     "grade": false,
     "grade_id": "cell-bcbe35b20c1007d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Loss function\n",
    "The objective function (variational lower bound), that we will use to train the VAE, consists of two terms: a log Bernoulli loss (reconstruction loss) and a KullbackLeibler divergence. We implement the two terms separately and combine them in the end.\n",
    "As seen in Part 1: Expectation Maximization, we can use a multivariate Bernoulli distribution to model the likelihood $p(\\bx | \\bz)$ of black and white images. Formally, the variational lower bound is maximized but in PyTorch we are always minimizing therefore we need to calculate the negative log Bernoulli loss and KullbackLeibler divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fb5f70b132e1233983ef89d19998374",
     "grade": false,
     "grade_id": "cell-389d81024af846e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.1 Negative Log Bernoulli loss (5 points)\n",
    "The negative log Bernoulli loss is defined as,\n",
    "\n",
    "\\begin{align}\n",
    "loss = - (\\sum_i^D \\bx_i \\log \\hat{\\bx_i} + (1  \\bx_i) \\log(1  \\hat{\\bx_i})).\n",
    "\\end{align}\n",
    "\n",
    "Write a function `log_bernoulli_loss` that takes a D dimensional vector `x`, its reconstruction `x_hat` and returns the negative log Bernoulli loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "952435ca03f47ab67a7e88b8306fc9a0",
     "grade": false,
     "grade_id": "cell-1d504606d6f99145",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def log_bernoulli_loss(x_hat, x):\n",
    "    \n",
    "    loss = x * torch.log(x_hat) + (1 - x) * torch.log(1 - x_hat)\n",
    "    loss = -torch.sum(loss)\n",
    "    \n",
    "    print(\"bern\", loss)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd2a490aa694507bd032e86d77fc0087",
     "grade": true,
     "grade_id": "cell-9666dad0b2a9f483",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(6.5239)\n",
      "bern tensor(6.5239)\n"
     ]
    }
   ],
   "source": [
    "### Test test test\n",
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 0.9, 0.9, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33, 0.44], [0.55, 0.66, 0.77, 0.88], [0.99, 0.99, 0.99, 0.99]])\n",
    "\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) > 0.0\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6b75b7a531ecc87bce57925c4da464ee",
     "grade": false,
     "grade_id": "cell-b3a7c02dee7aa505",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.2 Negative KullbackLeibler divergence (10 Points)\n",
    "The variational lower bound (the objective to be maximized) contains a KL term $D_{KL}(q(\\bz)||p(\\bz))$ that can often be calculated analytically. In the VAE we assume $q = N(\\bz, \\mu, \\sigma^2I)$ and $p = N(\\bz, 0, I)$. Solve analytically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d01a7e7fe2dcf5f1c5fb955b85c8a04a",
     "grade": true,
     "grade_id": "cell-4cab10fd1a636858",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "% TODO: Derivation\n",
    "\n",
    "$$ KL(q||p) = \\frac{1}{2} * \\sum_{j=1}^J (1 + log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "328115c94a66e8aba0a62896e647c3ba",
     "grade": false,
     "grade_id": "cell-c49899cbf2a49362",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Write a function `KL_loss` that takes two J dimensional vectors `mu` and `logvar` and returns the negative KullbackLeibler divergence. Where `logvar` is $\\log(\\sigma^2)$. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "33b14b79372dd0235d67bb66921cd3e0",
     "grade": false,
     "grade_id": "cell-125b41878005206b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def KL_loss(mu, logvar):\n",
    "    \n",
    "    loss = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar)\n",
    "    print(\"KL\", loss)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf72e196d2b60827e8e940681ac50a07",
     "grade": true,
     "grade_id": "cell-ba714bbe270a3f39",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL tensor(2.5450)\n",
      "KL tensor(2.5450)\n"
     ]
    }
   ],
   "source": [
    "### Test test test\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert KL_loss(mu_test, logvar_test) > 0.0\n",
    "assert KL_loss(mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "65335a588baac26bc48dd6c4d275fdca",
     "grade": false,
     "grade_id": "cell-18cb3f8031edec23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.3 Putting the losses together (5 points)\n",
    "Write a function `loss_function` that takes a D dimensional vector `x`, its reconstruction `x_hat`, two J dimensional vectors `mu` and `logvar` and returns the final loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6ecb5b60b2c8d7b90070ed59320ee70",
     "grade": false,
     "grade_id": "cell-d2d18781683f1302",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    \n",
    "    loss = log_bernoulli_loss(x_hat, x) + KL_loss(mu, logvar)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "816e9508408bfcb2c7332b508d505081",
     "grade": true,
     "grade_id": "cell-57747988d29bbb5d",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33], [0.44, 0.55, 0.66], [0.77, 0.88, 0.99]])\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) > 0.0\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4506e06ed44a0535140582277a528ba4",
     "grade": false,
     "grade_id": "cell-9e3ba708967fe918",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 The model\n",
    "Below you see a data structure for the VAE. The modell itself consists of two main parts the encoder (images $\\bx$ to latent variables $\\bz$) and the decoder (latent variables $\\bz$ to images $\\bx$). The encoder is using 3 fully-connected layers, whereas the decoder is using fully-connected layers. Right now the data structure is quite empty, step by step will update its functionality. For test purposes we will initialize a VAE for you. After the data structure is completed you will do the hyperparameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31eccf2f6600764e28eb4bc6c5634e49",
     "grade": false,
     "grade_id": "cell-e7d9dafee18f28a1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, fc1_dims, fc21_dims, fc22_dims, fc3_dims, fc4_dims):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*fc1_dims)\n",
    "        self.fc21 = nn.Linear(*fc21_dims)\n",
    "        self.fc22 = nn.Linear(*fc22_dims)\n",
    "        self.fc3 = nn.Linear(*fc3_dims)\n",
    "        self.fc4 = nn.Linear(*fc4_dims)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def decode(self, z):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "VAE_test = VAE(fc1_dims=(784, 4), fc21_dims=(4, 2), fc22_dims=(4, 2), fc3_dims=(2, 4), fc4_dims=(4, 784))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a2243397998b4f55c25dfd734f3e7e0",
     "grade": false,
     "grade_id": "cell-c4f9e841b8972a43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Encoding (10 points)\n",
    "Write a function `encode` that gets a vector `x` with 784 elements (flattened MNIST image) and returns `mu` and `logvar`. Your function should use three fully-connected layers (`self.fc1()`, `self.fc21()`, `self.fc22()`). First, you should use `self.fc1()` to embed `x`. Second, you should use `self.fc21()` and `self.fc22()` on the embedding of `x` to compute `mu` and `logvar` respectively. PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "628bcd88c611cf01e70f77854600199b",
     "grade": false,
     "grade_id": "cell-93cb75b98ae76569",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def encode(self, x):\n",
    "    \n",
    "    embed = self.fc1(x)\n",
    "    embed = F.sigmoid(embed)\n",
    "    \n",
    "    mu = self.fc21(embed)\n",
    "    logvar = self.fc22(embed)\n",
    "    \n",
    "    mu = F.relu(mu)\n",
    "    \n",
    "    return mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "370d930fa9f10f1d3a451f3805c04d88",
     "grade": true,
     "grade_id": "cell-9648960b73337a70",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.encode = encode\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "mu_test, logvar_test = VAE_test.encode(x_test)\n",
    "\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f597cc2b5ef941af282d7162297f865",
     "grade": false,
     "grade_id": "cell-581b4ed1996be868",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Reparameterization (10 points)\n",
    "One of the major question that the VAE is answering, is 'how to take derivatives with respect to the parameters of a stochastic variable?', i.e. if we are given $\\bz$ that is drawn from a distribution $q(\\bz|\\bx)$, and we want to take derivatives. This step is necessary to be able to use gradient-based optimization algorithms like SGD.\n",
    "For some distributions, it is possible to reparameterize samples in a clever way, such that the stochasticity is independent of the parameters. We want our samples to deterministically depend on the parameters of the distribution. For example, in a normally-distributed variable with mean $\\mu$ and standard deviation $\\sigma$, we can sample from it like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\bz = \\mu + \\sigma \\odot \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "where $\\odot$ is the element-wise multiplication and $\\epsilon$ is sampled from $N(0, I)$.\n",
    "\n",
    "\n",
    "Write a function `reparameterize` that takes two J dimensional vectors `mu` and `logvar`. It should return $\\bz = \\mu + \\sigma \\odot \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6331cb5dd23aaacbcf1a52cfecb1afaa",
     "grade": false,
     "grade_id": "cell-679aea8b2adf7ec4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def reparameterize(self, mu, logvar):\n",
    "            \n",
    "        eps = torch.randn_like(mu)\n",
    "        z = mu + torch.sqrt(torch.exp(logvar)) * eps\n",
    "        \n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38d4e047717ab334b262c8c177f0a420",
     "grade": true,
     "grade_id": "cell-fdd7b27a3d17f84e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.reparameterize = reparameterize\n",
    "VAE_test.train()\n",
    "\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "z_test = VAE_test.reparameterize(mu_test, logvar_test)\n",
    "\n",
    "assert np.allclose(z_test.size(), [3, 2])\n",
    "assert z_test[0][0] < 5.0\n",
    "assert z_test[0][0] > -5.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9241ab0eaf8366c37ad57072ce66f095",
     "grade": false,
     "grade_id": "cell-0be851f9f7f0a93e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Decoding (10 points)\n",
    "Write a function `decode` that gets a vector `z` with J elements and returns a vector `x_hat` with 784 elements (flattened MNIST image). Your function should use two fully-connected layers (`self.fc3()`, `self.fc4()`). PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8e833cfd7c54a9b67a38056d5d6cab8",
     "grade": false,
     "grade_id": "cell-bf92bb3878275a41",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def decode(self, z):\n",
    "    \n",
    "    x_hat = self.fc3(z)\n",
    "    x_hat = F.relu(x_hat)\n",
    "    x_hat = self.fc4(x_hat)\n",
    "    x_hat = F.relu(x_hat)\n",
    "    \n",
    "    return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7732293fd7d971fcf255496e8c68638d",
     "grade": true,
     "grade_id": "cell-4abb91cb9e80af5d",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test test test\n",
    "VAE.decode = decode\n",
    "\n",
    "z_test = torch.ones((5,2))\n",
    "x_hat_test = VAE_test.decode(z_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert (x_hat_test <= 1).all()\n",
    "assert (x_hat_test >= 0).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2e113d1f45398b2a1399c336526e755",
     "grade": false,
     "grade_id": "cell-97511fbc4f5b469b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.6 Forward pass (10)\n",
    "To complete the data structure you have to define a forward pass through the VAE. A single forward pass consists of the encoding of an MNIST image $\\bx$ into latent space $\\bz$, the reparameterization of $\\bz$ and the decoding of $\\bz$ into an image $\\bx$.\n",
    "\n",
    "Write a function `forward` that gets a a vector `x` with 784 elements (flattened MNIST image) and returns a vector `x_hat` with 784 elements (flattened MNIST image), `mu` and `logvar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8b7433c4631dd01c07a5fe287e55ae13",
     "grade": false,
     "grade_id": "cell-26bb463b9f98ebd5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = x.view(-1, 784)\n",
    "    mu, logvar = self.encode(x)\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    x_hat = self.decode(z)\n",
    "    return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8e7e495f40465c162512e9873c360b25",
     "grade": true,
     "grade_id": "cell-347e5fba3d02754b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test test test \n",
    "VAE.forward = forward\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "x_hat_test, mu_test, logvar_test = VAE_test.forward(x_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a114a6fd781fb949b887e6a028e07946",
     "grade": false,
     "grade_id": "cell-62c89e4d3b253671",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.7 Training (15)\n",
    "We will now train the VAE using an optimizer called Adam, https://arxiv.org/abs/1412.6980. The code to train a model in PyTorch is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3b6bb965fb48229c63cacda48baea65",
     "grade": false,
     "grade_id": "cell-be75f61b09f3b9b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(epoch, train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1, 784), mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48ca730dbef06a668f4dfdb24888f265",
     "grade": false,
     "grade_id": "cell-da1b063b7de850b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's train. You have to choose the hyperparameters. Make sure your loss is going down in a reasonable amount of epochs (around 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "846430258fb80f50b161135448726520",
     "grade": false,
     "grade_id": "cell-d4d4408d397f6967",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "fc1_dims = (784,200)\n",
    "fc21_dims = (200, 200)\n",
    "fc22_dims = (200, 200)\n",
    "fc3_dims = (200, 300)\n",
    "fc4_dims = (300, 784)\n",
    "lr = 1e-4\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b93390f399b743276bc25e67493344f2",
     "grade": true,
     "grade_id": "cell-ca352d8389c1809a",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains a hidden test, please don't delete it, thx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20719070ed85964de9722acc3456a515",
     "grade": false,
     "grade_id": "cell-5c77370db7cec9f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the box below to train the model using the hyperparameters you entered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38306be3638e85812bd5b2a052fcc0a4",
     "grade": false,
     "grade_id": "cell-5712d42de1068398",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(12225.8848)\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(12219.7422)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12212.5801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12204.4189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12197.5566)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12191.1006)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12182.8701)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12180.5244)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12178.4102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12179.7188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12175.4541)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12175.6289)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12175.6279)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12170.8975)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12172.7148)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12170.5459)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12170.3877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12168.7734)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12168.8223)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12167.8838)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12167.8965)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12164.9922)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12164.3057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12159.9688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12157.1104)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12156.1211)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12155.6816)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12153.8535)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12153.5195)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12149.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12147.8643)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12146.2285)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12142.9404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12145.2168)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12141.9121)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12140.2588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12140.2139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12137.9951)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12137.8604)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12136.1504)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12132.3896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12133.5391)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12133.1514)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12131.1660)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12131.4326)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12130.4551)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12128.2568)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12126.5127)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12127.3467)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12125.9824)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12125.9678)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12126.2139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12124.2383)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12126.1260)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12124.9883)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12127.2070)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12127.2627)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12124.7539)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12125.4961)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12125.4141)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12128.0654)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12128.3604)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12126.3135)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12127.8398)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12125.3682)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12123.0889)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12124.9932)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12123.8340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12124.7744)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12121.9248)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12123.5928)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12121.9873)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12125.0029)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12123.4355)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12120.9463)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12122.4834)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12122.2568)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12121.3730)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12120.6055)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12121.6572)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12120.7930)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12122.6025)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12121.1357)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12120.4365)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12119.0625)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12119.8926)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12117.9951)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12118.0986)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12117.2998)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12118.4072)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12119.1611)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12119.4258)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12117.8486)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12119.7793)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12122.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12120.1074)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12121.5098)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12118.5225)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12119.1035)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12120.9727)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12117.6582)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(12119.6416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12117.1064)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12118.2109)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12117.9189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12116.4609)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12116.9053)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12116.8330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12115.1572)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12113.4531)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12117.3896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12111.8145)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12113.2266)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12110.5635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12113.3574)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12112.8672)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12111.1738)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12110.3877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12112.4697)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12109.5020)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12111.9131)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12110.0996)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12109.2275)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12111.2695)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12109.7754)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12109.8076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12109.0625)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12106.6904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12108.5420)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12107.0645)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12105.7559)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12107.2061)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12106.7803)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12106.0957)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12104.7051)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12107.5107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.7021)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12105.3887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12103.5615)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12105.6904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12104.4756)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.2939)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12106.6777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.9697)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.8604)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.9785)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12103.3564)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12104.4385)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12103.8438)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.6133)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.0186)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12102.3330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12103.4131)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12103.9727)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12101.0029)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12103.0361)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12099.4443)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12101.4355)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12100.5586)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12099.9209)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12101.3477)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12099.8223)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12100.6084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12101.9512)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12098.1973)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12101.1846)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12099.5244)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12099.7080)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12098.5625)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12097.9463)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12099.6758)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12099.2930)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12097.9053)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12097.6143)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12096.4561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12100.2266)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12098.5586)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12098.4668)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12096.3486)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12098.0176)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12098.0576)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12097.1172)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12095.9912)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12095.3662)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12096.3945)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12097.7949)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12094.7109)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12094.8477)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12096.4189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12095.9590)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12096.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12097.2246)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12091.7852)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12093.8555)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12092.5918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12092.5078)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12091.3145)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12093.6221)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12095.9355)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12090.7998)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12090.7002)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(12092.6016)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(12093.9541)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12089.6875)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12093.7041)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12091.9512)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12091.6855)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12090.8359)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12092.0479)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12092.5977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12089.0068)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12091.9072)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12090.9844)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12089.3301)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12090.1426)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12089.2510)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12087.0635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12084.4102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12088.3838)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12088.6426)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12088.4590)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12089.4189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12089.3516)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12089.4053)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12086.5322)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12088.4033)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12086.5869)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12087.5352)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12084.7842)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12085.5801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12087.9648)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12085.5918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12086.4795)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12085.4141)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12084.9990)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12084.0137)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12082.9854)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12086.2637)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12085.6270)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12083.4180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12084.2715)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12081.5361)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12077.3564)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12078.9424)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12084.0225)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12082.6230)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12081.1514)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12080.0498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12080.4980)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12079.7686)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12079.7793)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12079.7275)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12077.4395)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12076.9287)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12078.7715)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12077.3447)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12078.0664)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12074.9893)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12076.6084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12076.1709)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12076.0498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12072.9229)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12073.9775)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12077.6934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12070.8760)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12074.0400)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12071.0146)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12074.9492)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12073.2695)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12070.4863)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12071.6494)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12070.3281)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12072.4062)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12073.3828)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12074.0518)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12069.7441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12068.9893)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12068.9258)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12071.2744)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12071.6729)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12069.1465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12073.3730)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12073.1592)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12072.4268)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12068.3848)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12069.9141)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12071.8799)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12070.6934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12068.5137)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12070.7393)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12067.8643)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12067.0361)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12070.6914)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12067.5518)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12068.0947)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12068.7871)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.4033)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12071.0283)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12067.1465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12066.8525)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12068.1621)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(12067.4912)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12064.5117)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12069.2188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.6162)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.8760)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12067.7500)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12069.0225)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12066.2871)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.8594)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12066.9287)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12064.5752)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.2490)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.8486)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.4688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.8789)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.6123)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.3154)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12064.7969)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.8691)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12064.1240)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.6465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.7637)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12064.2598)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.2031)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.8447)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.9492)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12061.9248)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.6963)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.0244)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.1328)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.1797)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.6514)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12064.4297)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12061.0664)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.1748)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12059.8857)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.6299)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.5830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.7285)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.1875)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.9160)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.1982)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.8965)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.6611)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12059.9404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.8418)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.7930)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.7617)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.6504)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.5498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12059.1328)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12061.2236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.4385)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.0469)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.7939)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.8652)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.0557)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12054.6992)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.6064)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.7520)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12059.1416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12059.8340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.9150)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.4717)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12061.8594)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.1875)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12062.2129)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.1172)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12063.0664)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12065.8516)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12059.7715)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.5000)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12061.4639)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12056.9961)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12060.1328)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12055.8018)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.8447)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12057.8516)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12053.8838)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12056.9629)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12055.6377)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12054.4502)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12052.8359)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12056.3936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12058.2070)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12055.5625)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12055.2861)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12053.3535)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12053.0596)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12055.0195)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12051.9111)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12051.7607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12051.5918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12053.3115)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12054.1924)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12051.5322)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12050.8633)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12052.1201)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12052.4189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12047.7920)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(12052.7617)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12050.2236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12048.3838)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12053.1533)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12053.3818)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12050.8652)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12052.1719)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(12046.6006)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12050.8135)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12049.1299)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12050.5059)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12047.2949)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12048.6104)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12049.1650)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12040.8330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12049.5674)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12046.9805)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12048.7705)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12045.6582)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12047.2168)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12050.5488)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12047.9912)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12047.5928)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.9336)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12045.3877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12048.2510)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12045.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12045.6309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12049.2471)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12048.6084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12044.5273)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.7510)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12045.8496)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12045.7158)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12047.0098)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12049.4785)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.6494)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12044.4707)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12046.8086)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12047.6406)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12041.2490)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12044.6494)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.0830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12044.9619)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.3428)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12040.6826)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.7070)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.1133)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12041.4150)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12037.1143)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12043.8906)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12040.6172)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12035.3398)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12039.8682)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12038.0713)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12037.4443)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12035.7324)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12034.7236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12040.2344)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12036.5723)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12037.2500)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12039.4941)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12031.2754)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12031.8887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12032.9092)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12032.0449)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12034.3555)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9021.3584)\n",
      "====> Epoch: 1 Average loss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(12033.6592)\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(12037.7939)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12028.2119)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12030.9990)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12030.7773)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12032.0820)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.6924)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.5635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12029.0547)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.8291)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12024.2412)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12035.4316)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.6045)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12029.4746)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12025.1074)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12030.6338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12030.9561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.8223)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12029.3076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12023.4414)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12027.0527)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12023.6748)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12024.6533)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12025.2480)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12020.5488)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12025.4854)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.6289)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12023.5850)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12025.6592)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.0029)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12019.9531)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12024.5273)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12026.9502)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12023.3799)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12019.3271)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12016.7988)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12020.5469)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12022.0879)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12021.6914)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12020.6309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12019.6416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12017.9043)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12019.9580)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12022.4717)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12013.2568)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12016.0342)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12015.8584)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12021.4238)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12017.9043)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12015.1221)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12013.3340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12013.9688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12014.1895)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12016.9072)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12017.5908)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12017.1455)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12014.4873)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12017.1270)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12013.2500)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12017.7354)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12015.7158)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12008.4688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12015.1807)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12013.0645)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12009.4189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12015.7578)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12007.4102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12008.5010)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12010.9863)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12010.9639)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12008.9082)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12007.7881)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12009.7510)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12006.3760)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12011.9062)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12008.9141)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12007.6338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12004.8076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12004.3984)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12005.1562)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12008.8584)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12001.4463)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12004.8271)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12003.7471)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11999.6865)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11996.4561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11999.7256)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12001.3408)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12003.1338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11999.3057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12002.0869)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11999.1152)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12000.2666)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12001.0234)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11998.3457)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11998.3076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11996.4092)\n",
      "bern tensor(nan.)\n",
      "KL tensor(12000.3311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11998.9150)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11996.9629)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11990.9355)\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11993.8652)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11992.8721)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11996.6758)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11997.0850)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11993.3057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11998.4961)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11993.0664)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11992.2783)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11990.8086)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11995.1719)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11997.4561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11987.1338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11989.2109)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11991.6152)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11995.0215)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11983.4404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11979.1328)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11984.9092)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11977.4395)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11974.4951)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11975.5869)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11969.9766)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11974.9727)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11976.0977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11966.6016)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11967.7900)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11965.6143)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11971.3564)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11964.3311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11966.0801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11963.6885)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11963.4404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11962.8604)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11961.7275)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11960.6592)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11966.8037)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11959.0283)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11960.2998)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11961.8076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11958.9434)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11956.6143)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11960.6416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11958.7021)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(11955.3408)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11961.2139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11955.7793)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11956.1631)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11957.0859)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11960.4404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11955.7295)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11955.2793)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11954.)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11949.1670)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11950.3613)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11959.8955)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11949.2598)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11951.0527)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11948.4883)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11950.8877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11946.1279)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11951.7910)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11950.7764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11944.5977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11954.4990)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11953.4346)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11951.4736)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11946.9229)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11946.8008)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11939.0029)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11941.0684)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11945.1465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11932.9658)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11945.0889)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11941.7168)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11945.3652)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11939.2529)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11940.9395)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11936.6035)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11942.7402)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11935.4961)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11941.3389)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11937.1201)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11934.8887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11937.2754)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11939.9688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11935.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11936.0928)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11935.6572)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11941.8398)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11937.6367)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11935.4639)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11936.3584)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11936.2812)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11934.9092)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11935.9453)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11928.9873)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11928.6855)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11929.8994)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11932.4854)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11927.4639)\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11930.2832)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11934.4082)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11932.1758)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11924.4619)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11927.6240)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11932.5830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11928.8643)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11924.0459)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11927.5127)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11928.0586)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11927.0361)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11925.4277)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11921.9277)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11921.5205)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11921.6006)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11920.2686)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11916.4189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11920.5430)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11920.1953)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11914.0908)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11913.2939)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11910.8555)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11914.7363)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11910.4258)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11905.9385)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11907.6865)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11906.7920)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11907.8096)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11909.6172)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11907.1582)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11902.8672)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11896.3027)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11910.2617)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11901.4521)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11902.9121)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11899.6309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11900.0254)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11901.3682)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11905.7891)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11896.3281)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11888.2959)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11899.8955)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11893.8867)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11900.7764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11903.3135)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11898.7158)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11895.7861)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.6416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.8066)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11892.0898)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11895.8252)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11889.7314)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11889.8994)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11895.7646)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11892.9697)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11895.5811)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11896.1797)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.2676)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11890.8350)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11887.6182)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11894.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11901.1260)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11896.1904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11901.1699)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11892.4346)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11892.4336)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11889.4688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11895.6699)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11888.4482)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.0332)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11894.5244)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11894.2705)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11888.9336)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11888.9678)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11889.8711)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11889.9521)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11880.2646)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11890.2656)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11890.7246)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11892.6270)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11898.0674)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11898.7100)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11890.0508)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11892.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11887.9326)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11886.8594)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.1523)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11893.6309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11885.8340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11886.1758)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11887.6055)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.6943)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11888.0615)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.1348)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11884.2480)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11880.7607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11891.0635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11887.7568)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11881.5029)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11884.4404)\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11882.2080)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11892.7588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11885.6221)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11885.7686)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11884.0098)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11885.8027)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11888.9482)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11879.5498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11886.3076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11881.3369)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11876.2842)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11888.2041)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11878.0469)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11875.4893)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11876.3018)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11877.2207)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11874.7881)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11870.4170)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11871.6875)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11872.5010)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11875.1963)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11876.4473)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11878.6523)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11870.6006)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11874.5361)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11870.8252)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11870.0361)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11870.2822)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11872.8799)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11869.4941)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11875.2451)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11879.3955)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11876.2207)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11869.5762)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11869.5693)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11869.7588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11873.8896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11868.8730)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11875.3037)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11870.5186)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11880.6436)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11867.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11870.0938)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11868.6885)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11869.8926)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11869.4600)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11861.9795)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(11858.2578)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11867.2461)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11874.1484)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11863.9814)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11863.9824)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11859.3877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11860.0518)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11863.6025)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11861.7236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11864.0586)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11864.5820)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11853.1748)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11860.8008)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11860.7764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11859.8740)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11864.4150)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11866.3467)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11854.8896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11859.4268)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11866.2070)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11861.0234)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11860.8887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11861.2129)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11847.0127)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11852.8965)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11853.1309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11846.6191)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11846.8877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11843.6631)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11845.8125)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11835.0918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11835.1904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11837.4863)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11833.2344)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11839.6777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11839.5137)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11836.0654)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11835.4062)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11834.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11835.1650)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11828.0146)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11840.1836)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11824.0342)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11824.2969)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11832.9883)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11835.6797)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11825.6074)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11823.5342)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11824.7764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11828.6846)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11821.6182)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11821.4180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11833.0879)\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11829.2510)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11825.3223)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11816.7891)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11834.3320)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11827.5254)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11828.5615)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11822.8350)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11825.7910)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11823.7041)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11821.0889)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11824.6240)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11822.2881)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11826.0840)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11825.4619)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11829.0801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11821.2402)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11823.0537)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11821.7979)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11819.3135)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11825.9189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11820.4287)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11817.8789)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11822.9053)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11820.6289)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11809.8662)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11820.2168)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11816.8955)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11818.3340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11820.1934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11810.0635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11804.7188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11818.4756)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11816.0332)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11813.6260)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11814.5918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11808.5977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11808.0791)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11809.)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11810.5020)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11812.7900)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11810.1855)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11804.8594)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11802.1318)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11802.9912)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11804.3057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11811.6123)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11807.0381)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11800.1816)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11797.3438)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11803.9102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11813.8906)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11799.5332)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11806.2051)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11801.2129)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11806.9414)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11806.1836)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11791.0977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11803.3672)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11803.6738)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11799.5449)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11796.3984)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11800.7617)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11793.9502)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11799.7754)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11796.8535)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11806.4053)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11789.6660)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8848.6084)\n",
      "====> Epoch: 2 Average loss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11792.9082)\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11791.8809)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11792.0527)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11798.8311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11785.1299)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11795.8447)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11785.3467)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11783.8623)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11792.5967)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11789.5391)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11792.1182)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11787.5947)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11783.3975)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11785.9775)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11795.3184)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11786.9277)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11783.7480)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11789.0264)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11792.0020)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11779.5264)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11790.8838)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11786.1416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11792.3535)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11784.7627)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11793.6963)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11782.3887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11782.7793)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11787.3984)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11777.4180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11772.7588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11785.4531)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11785.6396)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11777.6338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11777.8164)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11774.2412)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11780.3916)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11785.7793)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11780.3955)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11775.6992)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11775.4785)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11779.2012)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11777.0977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11771.8916)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11776.0391)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11769.7393)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11771.2695)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11786.0156)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11771.6943)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11771.2012)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11773.6133)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11775.5723)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11774.1338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11779.4014)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11773.1621)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11769.1709)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11763.7617)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11764.0381)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11754.2578)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11754.3008)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11753.5947)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11751.7441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11750.1094)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11751.1348)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11738.3076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11745.1055)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11743.5488)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11747.2607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11740.4180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11741.4541)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11728.2900)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11727.9062)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11740.2754)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11731.8203)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11734.7666)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11729.4258)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11727.5654)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11733.4102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11733.9473)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11722.1768)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11718.2568)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11728.8584)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11716.0820)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11723.1162)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(11720.7119)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11723.2969)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11716.5557)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11725.6201)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11719.6973)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11720.8672)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11721.5928)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11717.9668)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11717.6211)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11709.7402)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11719.7988)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11724.0635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11713.3340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11721.1611)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11707.8281)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11714.8057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11718.6084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11718.4629)\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11704.9844)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11718.3721)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11713.7041)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11710.2266)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11705.5273)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11708.1748)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11706.9541)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11715.8330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11711.5967)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11711.9990)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11712.6748)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11709.2119)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11710.9180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11719.4326)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11706.9570)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11703.4238)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11691.4404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11715.7734)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11714.8643)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11703.8291)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11702.2354)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11708.6533)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11706.5176)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11690.1162)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11694.8213)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11694.2061)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11698.8711)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11702.4453)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11689.8066)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11697.3926)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11709.2041)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11690.3965)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11702.3369)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11703.4951)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11694.0693)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11701.4580)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11690.3252)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11691.5732)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11693.9717)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11692.3691)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11701.2461)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11686.7852)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11688.1191)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11698.9160)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11686.0283)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11698.0293)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11677.)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11686.9385)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11700.5498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11690.8584)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11699.7949)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11693.2080)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11672.3623)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11677.2314)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11672.9521)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11687.0264)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11680.5742)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11679.5293)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11681.7578)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11677.5400)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11679.8311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11671.6377)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11685.1240)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11667.6426)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11681.9414)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11670.6387)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11669.0762)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11675.9326)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11681.2275)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11666.6641)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11677.2705)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11679.7861)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11677.3916)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11664.9160)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11669.4277)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11683.0566)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11668.0918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11654.7676)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11674.6699)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11657.3916)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11675.6592)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11666.2295)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11664.1631)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11660.8613)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11665.6582)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11643.1787)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11660.9590)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11652.9873)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11654.3408)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11663.9795)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11654.3652)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11651.4141)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11658.5566)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11649.8311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11650.9355)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11653.7266)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11658.0596)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11659.6416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11661.7080)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11645.6865)\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11650.5830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11642.6133)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11645.6221)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11646.8281)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11664.2217)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11645.8457)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11650.3408)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11646.6064)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11636.5410)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11638.3066)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11646.8135)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11660.8750)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11647.3125)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11648.7881)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11640.3457)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11638.1650)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11652.2559)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11647.2266)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11635.6523)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11652.3008)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11636.4102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11652.0303)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11632.3770)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11631.8184)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11627.3174)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11643.0283)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11635.5381)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11637.1172)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11630.5498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11641.7412)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11629.7764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11618.9531)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11624.3027)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11636.9424)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11622.4004)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11619.9883)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11619.8877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11625.2344)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11622.2549)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11631.1992)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11636.9951)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11613.4863)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11625.1455)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11626.7178)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11632.3916)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11639.0381)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11626.7090)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11626.2939)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11622.7275)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11640.4385)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11615.5771)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11619.8770)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11626.0283)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11621.8262)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11632.6709)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11647.6240)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11625.5352)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11623.7812)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11624.6016)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11627.8516)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11642.1201)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11634.4746)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11615.2373)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11623.9111)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11614.2119)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11626.1895)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11624.2773)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11620.8770)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11629.1484)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11624.5176)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11614.6279)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11621.7998)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11621.8740)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11606.0498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11611.0068)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11619.1729)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11622.5469)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11619.2900)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11596.3662)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11629.5410)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11622.8926)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11605.6182)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11603.3525)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11617.4355)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11613.4111)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11608.3613)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(11599.7930)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11613.4854)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11601.0156)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11612.4727)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11612.3008)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11610.5205)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11602.9902)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11600.7178)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11599.8887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11605.2139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11605.7490)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11598.2158)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11613.8975)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11606.2764)\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11600.0107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11594.9648)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11606.8359)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11607.2852)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11588.1514)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11597.5615)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11584.5723)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11592.4258)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11583.0127)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11599.9834)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11588.3799)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11587.8818)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11595.0332)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11580.2529)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11580.7959)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11583.4551)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11593.0488)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11571.4482)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11586.1934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11592.5420)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11573.4033)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11584.0391)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11569.5293)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11581.5898)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11587.4404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11589.2275)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11574.6289)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11573.3535)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11582.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11584.1299)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11569.3262)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11579.7451)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11563.1025)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11586.9268)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11581.8926)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11567.3174)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11576.3887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11574.7490)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11569.7539)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11579.0977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11567.0352)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11571.6797)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11579.1250)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11577.0439)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11575.6787)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11580.9648)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11574.2949)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11564.7412)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11568.4219)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11572.7734)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11575.2959)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11572.5205)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11567.2236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11582.4707)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11574.1748)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11560.2607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11574.0918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11572.6377)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11572.6777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11568.1982)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11567.0127)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11562.4443)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11565.1465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11551.5771)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11560.4424)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11553.5342)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11560.3652)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11567.1709)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11561.1016)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11565.7422)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11555.6055)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11573.3418)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11564.6895)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11559.2842)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11553.8789)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11555.7090)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11559.6641)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11550.6953)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11567.4766)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11547.4375)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11556.4893)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11559.0820)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11547.7900)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11552.6670)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11534.0215)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11552.4980)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11523.5430)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11554.4355)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11539.5479)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11553.0332)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11542.2871)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11538.9863)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11533.2549)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11566.7217)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11538.0732)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11547.0137)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11539.4482)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11530.3193)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11557.2002)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11532.5479)\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11545.8340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11539.6885)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11535.4736)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11542.4033)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11531.7480)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11536.3867)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11541.3193)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11528.2109)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11529.4766)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11545.5303)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11542.7852)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11541.4736)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11538.0234)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11542.6904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11532.0312)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11540.0967)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11558.2842)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11544.9541)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11547.8584)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11524.6807)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11518.9824)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11539.1846)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11550.7637)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11530.1250)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11539.2539)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11525.4541)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11543.9121)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11554.1494)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11529.0156)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11516.9219)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11540.6973)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11537.2432)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11513.1602)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11531.3340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11527.2275)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11531.2627)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11533.6797)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11521.5947)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11519.3936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11526.4668)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11510.4170)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11522.5254)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11526.2412)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11517.0508)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11518.5938)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11510.5088)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11516.7988)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11506.6445)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11515.6982)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11534.8311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11502.9717)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11500.0830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11514.6680)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11519.8682)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11510.3066)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11499.9004)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11512.7012)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11514.8975)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11501.0420)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11495.2441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11498.8789)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11494.9971)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11494.5371)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11495.2178)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11499.0566)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11481.7549)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11494.7871)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8615.3242)\n",
      "====> Epoch: 3 Average loss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11483.0566)\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11475.1309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11485.0830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11481.2236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11489.6602)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11484.1729)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11474.6084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11461.3350)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11484.6016)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11468.0801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11461.5791)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11452.0762)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11469.8428)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11461.8701)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11479.1982)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11463.4824)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11475.7314)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11470.3594)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11468.6055)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11463.1270)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(11454.7119)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11463.6396)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11460.7324)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11459.6768)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11444.5273)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11472.2783)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11472.9854)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11435.7764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11443.5293)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11445.1865)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11442.9014)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11444.6465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11449.9863)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11455.1523)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11448.5869)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11449.2373)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11450.7480)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11443.4834)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11433.2002)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11438.6387)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11427.9902)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11446.5039)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11422.8350)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11426.6084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11407.2842)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11443.2539)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11420.6230)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11433.7148)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11426.6670)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11427.0039)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11430.8721)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11427.7031)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11417.3037)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11432.0713)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11405.8330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11430.6904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11414.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11414.6777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11409.0996)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11415.4375)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11410.1807)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11420.9443)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11403.7910)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11417.9971)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11399.5791)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11410.9590)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11420.8955)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11416.2441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11423.0664)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11419.6719)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11409.7314)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11425.3896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11404.2090)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11418.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11413.2080)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11413.9727)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11402.7422)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11412.3818)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11387.8877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11419.0303)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11416.0234)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11389.6777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11405.0107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11390.6621)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11391.7627)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11392.5957)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11393.0381)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11404.1338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11385.)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11399.5371)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11399.6553)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11404.3154)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11382.9688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11395.3721)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11391.8252)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11383.1240)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11378.5801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11383.4941)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11380.7109)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11389.5801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11381.5801)\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11377.4600)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11383.7930)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11380.4990)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11372.8564)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11370.6055)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11375.7031)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11384.2529)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11390.6279)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11370.9307)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11357.8730)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11365.1084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11373.1445)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11357.6885)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11358.0303)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11380.5215)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11360.1807)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11365.4424)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11349.7373)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11369.3408)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11359.7129)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11351.5596)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11350.0078)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11355.0107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11351.4795)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11367.8105)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11335.3584)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11349.8711)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11344.6797)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11351.4072)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11332.5977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11327.3662)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11335.5615)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11320.2812)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11337.2236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11339.3916)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11341.1748)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11294.6377)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11324.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11323.6484)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11320.3809)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11307.1504)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11290.3506)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11292.3350)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11295.5215)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11291.5186)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11278.0068)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11295.3008)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11297.8291)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11284.5859)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11312.7871)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11301.8145)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11310.5928)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11286.8779)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11300.9609)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11318.8477)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11309.9395)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11299.5840)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11295.4297)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11288.0674)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11307.0312)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11307.0244)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11305.3945)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11304.9180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11282.3945)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11303.4561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11293.1738)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11301.7061)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11282.9375)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11270.3193)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11276.9658)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11286.1680)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11255.1777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11297.1943)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11298.4111)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11255.9189)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11295.2422)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11277.6328)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11270.7324)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11281.8760)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11273.0352)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11301.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11272.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11268.7666)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11291.9561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11283.5557)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11296.5088)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11276.7354)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11247.5244)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11270.0137)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11275.7725)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11253.2891)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11254.2910)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11278.8730)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11246.1455)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11274.8965)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11260.7012)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11243.7646)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11252.0986)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11254.6064)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11271.2158)\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11270.5850)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11255.2490)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11243.7285)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11237.8389)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11275.5771)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11249.5352)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11248.7891)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11261.0479)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11253.1133)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11249.4072)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11252.1152)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11255.3750)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11254.1191)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11250.2588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11247.4971)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11214.2910)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11221.2715)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11237.8877)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11244.5215)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11236.9209)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11246.9932)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11250.8350)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(11212.7627)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11232.5596)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11219.3330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11211.7090)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11220.1826)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11224.4990)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11227.8223)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11224.2080)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11209.0322)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11205.2773)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11211.0908)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11199.8896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11225.7617)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11211.4062)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11224.1562)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11206.6250)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11225.0811)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11202.0234)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11200.8291)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11205.7686)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11216.5762)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11208.9746)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11203.9570)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11201.6992)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11185.0449)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11198.4512)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11203.4893)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11179.5625)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11190.0527)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11168.7402)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11199.9834)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11186.4443)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11198.7314)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11192.4668)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11196.3057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11182.0264)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11163.8193)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11179.5996)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11163.2295)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11165.7705)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11192.6133)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11166.1143)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11175.8389)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11186.0557)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11166.0898)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11155.0732)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11171.6787)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11158.7822)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11144.6436)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11162.0244)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11149.8701)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11143.6025)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11159.3145)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11164.1523)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11157.3848)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11147.3154)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11138.7256)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11154.4180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11120.6230)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11105.3906)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11120.9404)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11128.2158)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11124.2266)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11122.4814)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11087.3555)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11066.9209)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11074.8311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11064.6123)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11099.5137)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11075.3193)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11074.1221)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11089.8311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11066.8818)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11061.8633)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11041.2070)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11062.2070)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11028.9365)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11036.0420)\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(11036.3408)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11040.4248)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11013.3701)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11033.5449)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11003.9248)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11028.3789)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11015.7773)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11014.4678)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10991.4746)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11021.1025)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10997.0029)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11024.3447)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11014.5107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10990.0898)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10999.9717)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11026.2754)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10981.1660)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10998.7588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10987.7236)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11003.2920)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10974.1934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10999.7197)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11002.6553)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10985.5742)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10994.0703)\n",
      "bern tensor(nan.)\n",
      "KL tensor(11009.2500)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10988.7881)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10972.5557)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10974.8398)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10951.1875)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10976.4814)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10991.3330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10976.6582)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10943.9600)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10937.5576)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10973.1279)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10960.6777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10943.3135)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10978.7969)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10932.9395)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10936.6123)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10932.9482)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10927.1572)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10944.1777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10935.8330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10917.5586)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10963.4883)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10915.1777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10914.5869)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10916.4424)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10906.4658)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10873.6631)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10898.4521)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10919.6436)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10886.7109)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10863.2598)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10878.4502)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10879.6934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10825.7988)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10850.1494)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10885.3828)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10847.3750)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10817.9248)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10821.2500)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10855.1777)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10801.0098)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10822.5215)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10813.2412)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10805.9736)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10801.9492)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10798.3369)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10792.3613)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10787.6152)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10775.8447)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10788.9766)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10800.0850)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10775.5156)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10794.7715)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10764.7920)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10743.6689)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10803.0576)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10749.5186)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10761.4824)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10731.1309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10787.6611)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10747.1357)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10732.4766)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10741.7002)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10715.6553)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10736.3340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10748.4590)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10756.2822)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10740.9814)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10731.0146)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10715.)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10717.6504)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10711.3301)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10699.3770)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10724.4268)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10727.2256)\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(10693.8564)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10705.7559)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10670.1465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10671.6719)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10713.7100)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10698.0938)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10685.5674)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10661.1973)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10684.8994)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10691.0518)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10700.2705)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10684.1064)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10662.4297)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10667.0625)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10630.6074)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10682.0742)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10675.9648)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10678.8564)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10685.7285)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10672.7900)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10681.8857)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10661.0811)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10640.8926)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10641.7139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10649.6973)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(10631.4561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10603.5352)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10593.8203)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10617.3467)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10626.8770)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10619.4922)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10635.0977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10628.2422)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10573.3965)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10621.0703)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10625.4912)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10643.8535)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10598.8594)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10638.4932)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10592.0283)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10612.1426)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10602.9453)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10607.2744)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10534.9297)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10595.8389)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10600.4932)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10557.9160)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10573.7812)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10545.8750)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10564.6377)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10584.4434)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10549.1211)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10532.5459)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10559.9727)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10553.0703)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10580.4727)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10516.9160)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10542.3086)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10545.2217)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10531.8799)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10555.7441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10510.7188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10577.8506)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10505.7842)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10499.1445)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10500.4629)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10545.0107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(7897.5078)\n",
      "====> Epoch: 4 Average loss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(10537.1426)\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(10517.2461)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10550.6299)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10509.3799)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10516.9570)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10495.0537)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10544.1787)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10551.5967)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10481.4531)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10519.3066)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10472.9688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10503.3330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10503.7031)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10469.7188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10470.7871)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10478.0830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10476.1250)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10496.0225)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10431.7764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10448.2852)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10444.2842)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10435.6318)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10431.2656)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10426.0996)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10468.5977)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10462.2148)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10478.8496)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10455.5098)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10460.9414)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10395.2305)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10458.4424)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10433.4775)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10426.0713)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10444.7441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10406.2920)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10439.5762)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10384.1514)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10421.5811)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10459.0576)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10419.9824)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10439.3057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10403.2607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10387.2480)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10424.8164)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10450.6016)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10365.2871)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10406.2188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10415.8516)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10366.4717)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10405.4268)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10401.3896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10379.2383)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10396.9570)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10385.9199)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10358.1650)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10438.4453)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10350.8262)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10385.0801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10356.9736)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10362.2715)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10373.1318)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10326.3711)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10334.1738)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10347.4561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10376.9385)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10365.0928)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10321.4053)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10327.6299)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10351.4170)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10352.3271)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10342.2441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10348.1992)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10330.5752)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10346.8340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10311.1953)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10301.4648)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10292.7119)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10310.8252)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10304.2334)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10283.6689)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10288.3252)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10288.5322)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10309.3545)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10271.4746)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10233.2441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10249.2891)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10239.7939)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10267.1387)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10235.7305)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10231.7012)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10242.8027)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10215.7441)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10229.9512)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10185.1064)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10218.7207)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10215.7109)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10212.8643)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10260.4102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10176.8369)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10220.9033)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10199.3008)\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(10210.8291)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10182.3896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10178.8330)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10197.6533)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10160.5137)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10178.1338)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10155.5674)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10176.1699)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10181.6904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10187.0449)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10156.4561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10124.8838)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10123.9990)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10142.4414)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10137.6221)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10141.5107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10134.6240)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10123.5684)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10176.3789)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10153.1992)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10109.6279)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10038.1934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10088.2539)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10115.6973)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10106.4707)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10118.0566)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10092.4619)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10071.0859)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10060.3516)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10045.2754)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10034.3477)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10084.6582)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10059.6699)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10087.8428)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10049.2607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10055.0205)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9997.0957)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10045.7559)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10018.5479)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10000.6416)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10000.2510)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9996.2764)\n",
      "bern tensor(nan.)\n",
      "KL tensor(10021.3652)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9908.4521)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9976.5352)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9981.7588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9941.3418)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9974.9219)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9966.6396)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9983.7637)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9948.0166)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9979.5117)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9949.6924)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9960.7686)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9965.9043)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9911.0801)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9950.9229)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9948.5898)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9920.5449)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9894.9980)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(9946.9521)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9912.7646)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9906.9932)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9932.0586)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9904.2529)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9900.1104)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9885.2686)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9938.4248)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9885.1182)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9868.5811)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9879.4043)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9774.2305)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9898.2910)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9780.1084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9828.4707)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9837.2607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9855.1680)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9832.7158)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9803.3496)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9778.4053)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9847.3223)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9757.2354)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9770.5732)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9775.4170)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9747.2188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9786.4209)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9802.1221)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9751.8643)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9747.2354)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9726.4941)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9733.4033)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9779.3066)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9745.3125)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9804.0840)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9677.3311)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9717.9980)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9705.9912)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9696.7246)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9747.2676)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9695.5957)\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(9736.2168)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9662.6084)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9707.4023)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9729.9258)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9748.0781)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9657.5176)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9717.5586)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9649.9180)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9627.8896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9685.4414)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9651.3145)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9640.4297)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9728.8242)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9678.4551)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9630.3232)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9607.8174)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9599.2295)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9646.8926)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9619.4482)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9635.0635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9621.8398)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9546.0518)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9615.5205)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9565.5938)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9676.2383)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9606.4492)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9616.5156)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9624.1592)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9581.4043)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9599.3975)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9630.9131)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9584.0107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9584.8496)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9546.2812)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9489.1465)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9514.1768)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9525.8867)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9564.1270)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9505.1055)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9535.1738)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9574.7227)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9544.1992)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9497.0684)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9542.0117)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9471.4932)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9536.5000)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9520.5684)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9438.5420)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9445.5127)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9375.7363)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9572.8643)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9466.9795)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9417.7178)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9448.3262)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9442.1963)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9457.8066)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9415.2656)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9473.9424)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9424.8047)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9410.6074)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9414.2939)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9361.1904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9381.4834)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9370.4873)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9379.8506)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9385.7559)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9352.9336)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9372.1201)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9361.1953)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9344.4111)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9320.8281)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9368.9385)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9294.9375)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9402.5146)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9353.1768)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9328.8887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9370.5039)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9369.4102)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9307.2432)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9268.6396)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9336.5361)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9334.0391)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9279.1484)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9289.3398)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9361.0889)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9269.9668)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9341.5771)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9307.0957)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9286.0391)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9293.7891)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9302.8057)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9277.0615)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9311.2588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9299.3760)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9217.6143)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9260.8916)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9311.9795)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9270.7803)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9267.9141)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9257.8906)\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(9165.7256)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9277.7998)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9202.8271)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9213.2266)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9182.2656)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9218.2148)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9211.2959)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9286.4619)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9273.6934)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9230.4609)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9196.0742)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9198.8936)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9195.6738)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9129.8545)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9165.1885)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9111.6982)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9176.5830)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9182.2256)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9171.9531)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9131.6152)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9149.8223)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9225.7100)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9015.6025)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9200.7021)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9077.9258)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9100.7344)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9127.2666)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9133.8887)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9132.1904)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8992.3896)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9047.8408)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9068.2461)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9146.7812)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9033.4834)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8989.4082)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9003.6680)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9074.6309)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9109.9590)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8914.5176)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9088.9688)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9059.0107)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8977.8135)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9029.9453)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9022.3438)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8933.3701)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8949.5117)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8981.4043)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8966.4473)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8996.2959)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8983.0498)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9000.6982)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8940.8076)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8941.0703)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9011.2598)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8954.0234)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8987.4287)\n",
      "bern tensor(nan.)\n",
      "KL tensor(9020.2588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8947.8604)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8899.5762)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8899.0283)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8898.1816)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8869.7393)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8926.7324)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8878.8535)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8866.3867)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8762.1172)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8933.5635)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8866.2959)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern tensor(nan.)\n",
      "KL tensor(8755.3691)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8736.3984)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8774.0703)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8719.4473)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8755.6006)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8707.6982)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8816.4141)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8729.5791)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8781.6182)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8712.6826)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8732.2139)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8746.7227)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8651.8203)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8759.3984)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8767.6367)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8747.3486)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8700.9580)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8762.6836)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8669.8750)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8729.3828)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8701.3145)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8639.9082)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8634.8760)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8690.7529)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8596.0996)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8590.7910)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8662.1924)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8636.9580)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8634.5234)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8553.9844)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8618.7607)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8602.1094)\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: nan\n",
      "bern tensor(nan.)\n",
      "KL tensor(8611.0898)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8460.9834)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8502.3340)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8646.3789)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8506.9092)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8642.7188)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8505.8125)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8559.7178)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8615.9170)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8539.0996)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8458.0059)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8469.2715)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8479.9883)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8483.5195)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8522.5176)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8466.6572)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8507.5918)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8428.2627)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8422.8447)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8464.2920)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8451.7852)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8402.1289)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8374.0820)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8314.0527)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8282.7168)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8389.8486)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8430.4268)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8439.5781)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8362.9395)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8358.1377)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8336.3457)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8327.6670)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8335.5693)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8332.4629)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8296.8984)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8333.5596)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8334.4531)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8349.8184)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8271.2529)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8291.3633)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8202.2246)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8251.1230)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8255.8389)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8298.8105)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8205.7295)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8185.9561)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8205.3320)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8307.2568)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8226.0479)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8116.7681)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8166.5073)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8163.0317)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8157.0464)\n",
      "bern tensor(nan.)\n",
      "KL tensor(7984.7935)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8075.8569)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8132.5020)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8159.7637)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8066.2295)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8162.7588)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8084.7505)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8120.0864)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8096.3389)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8058.6675)\n",
      "bern tensor(nan.)\n",
      "KL tensor(7997.7905)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8070.8394)\n",
      "bern tensor(nan.)\n",
      "KL tensor(8013.0088)\n",
      "bern tensor(nan.)\n",
      "KL tensor(7994.4717)\n",
      "bern tensor(nan.)\n",
      "KL tensor(6048.7280)\n",
      "====> Epoch: 5 Average loss: nan\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, shuffle=True, **{})\n",
    "\n",
    "# Init model\n",
    "VAE_MNIST = VAE(fc1_dims=fc1_dims, fc21_dims=fc21_dims, fc22_dims=fc22_dims, fc3_dims=fc3_dims, fc4_dims=fc4_dims)\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam(VAE_MNIST.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader, VAE_MNIST, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2f8fcc9384e30cb154cf931f223898b",
     "grade": false,
     "grade_id": "cell-bd07c058c661b9c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the box below to check if the model you trained above is able to correctly reconstruct images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "80d198e03b1287741d761a12e38dcf73",
     "grade": false,
     "grade_id": "cell-df03d717307a6863",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Let's check if the reconstructions make sense\n",
    "# Set model to test mode\n",
    "VAE_MNIST.eval()\n",
    "    \n",
    "# Reconstructed\n",
    "train_data_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_plot = torch.utils.data.DataLoader(train_data_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader_plot):\n",
    "    x_hat, mu, logvar = VAE_MNIST(data)\n",
    "    plt.imshow(x_hat.view(1,28,28).squeeze().data.numpy(), cmap='gray')\n",
    "    plt.title('%i' % train_data.train_labels[batch_idx])\n",
    "    plt.show()\n",
    "    if batch_idx == 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7f559122b150f5f1228d6b66b62f462c",
     "grade": false,
     "grade_id": "cell-76649d51fdf133dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Visualize latent space (20 points)\n",
    "Now, implement the auto-encoder now with a 2-dimensional latent space, and train again over the MNIST data. Make a visualization of the learned manifold by using a linearly spaced coordinate grid as input for the latent space, as seen in  https://arxiv.org/abs/1312.6114 Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c879ffdb0d355349d7144a33d16ca93a",
     "grade": true,
     "grade_id": "cell-4a0af6d08d055bee",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9eb1684d646eea84a25638d184bfbda",
     "grade": false,
     "grade_id": "cell-dc5e1247a1e21009",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Amortized inference (10 points)\n",
    "What is amortized inference? Where in the code of Part 2 is it used? What is the benefit of using it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "364ed922da59070f319d0bdfb0e41d92",
     "grade": true,
     "grade_id": "cell-6f7808a9b0098dbf",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
